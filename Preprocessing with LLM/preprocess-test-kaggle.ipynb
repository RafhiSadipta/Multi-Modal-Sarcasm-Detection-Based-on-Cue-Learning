{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9560502,"sourceType":"datasetVersion","datasetId":5826041}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- ENVIRONMENT & LIBRARY SETUP ---\nimport os\nimport sys\nimport pandas as pd\nimport time\nfrom tqdm.notebook import tqdm\nimport ast\n\nprint(\"=== TEST DATA PREPROCESSING SETUP ===\")\nprint(f\"Python version: {sys.version.split()[0]}\")\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Validate Kaggle environment\nassert '/kaggle/' in os.getcwd(), \"This notebook must run in Kaggle environment!\"\nprint(\"âœ“ Confirmed running in Kaggle environment\")\n\n# Install required packages\nprint(\"Installing OpenAI package...\")\n!pip install -q openai\n\nfrom openai import OpenAI\nprint(\"âœ“ Libraries imported successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:08.876091Z","iopub.execute_input":"2025-06-19T15:37:08.876376Z","iopub.status.idle":"2025-06-19T15:37:16.355149Z","shell.execute_reply.started":"2025-06-19T15:37:08.876350Z","shell.execute_reply":"2025-06-19T15:37:16.354058Z"}},"outputs":[{"name":"stdout","text":"=== TEST DATA PREPROCESSING SETUP ===\nPython version: 3.11.11\nWorking directory: /kaggle/working\nâœ“ Confirmed running in Kaggle environment\nInstalling OpenAI package...\nâœ“ Libraries imported successfully\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- API SETUP ---\nprint(\"Setting up OpenRouter API...\")\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"OPENROUTER_API_KEY\")\n    if api_key:\n        print(\"âœ“ OpenRouter API key loaded from Kaggle Secrets\")\n    else:\n        raise ValueError(\"No API key found\")\nexcept Exception as e:\n    print(f\"âŒ ERROR: Could not load API key: {e}\")\n    print(\"Please add OPENROUTER_API_KEY to Kaggle Secrets\")\n    raise\n\n# Initialize OpenRouter client\nopenrouter_client = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=api_key,\n)\n\nprint(\"âœ“ OpenRouter client initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:16.357205Z","iopub.execute_input":"2025-06-19T15:37:16.357608Z","iopub.status.idle":"2025-06-19T15:37:17.071548Z","shell.execute_reply.started":"2025-06-19T15:37:16.357581Z","shell.execute_reply":"2025-06-19T15:37:17.070646Z"}},"outputs":[{"name":"stdout","text":"Setting up OpenRouter API...\nâœ“ OpenRouter API key loaded from Kaggle Secrets\nâœ“ OpenRouter client initialized\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- DATA LOADING ---\nprint(\"Loading test data...\")\n\n# ğŸ§ª TESTING MODE - Set to True for small scale testing\nTESTING_MODE = True  # Change to False for full processing\nTEST_SAMPLE_SIZE = 20  # Number of samples for testing\n\n# Find dataset\nkaggle_data_paths = [\n    \"/kaggle/input/data-of-multimodal-sarcasm-detection\",\n]\n\ndata_dir = None\nfor path in kaggle_data_paths:\n    if os.path.exists(path):\n        data_dir = path\n        print(f\"âœ“ Dataset found at: {data_dir}\")\n        break\n\nif data_dir is None:\n    print(\"âŒ ERROR: Dataset not found!\")\n    print(\"Please add the multimodal sarcasm detection dataset to this notebook\")\n    raise FileNotFoundError(\"Dataset not found\")\n\ndef load_test_data(filepath):\n    \"\"\"Load test data efficiently\"\"\"\n    records = []\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"Test file not found: {filepath}\")\n        \n    print(f\"Loading {filepath}...\")\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line_num, line in enumerate(f, 1):\n            try:\n                data_list = ast.literal_eval(line.strip())\n                if len(data_list) >= 3:\n                    records.append({\n                        'id': data_list[0], \n                        'text': data_list[1], \n                        'sarcasm': int(data_list[2])\n                    })\n            except (ValueError, SyntaxError, IndexError):\n                continue\n    \n    print(f\"âœ“ Loaded {len(records)} test records\")\n    return pd.DataFrame(records)\n\n# Load test data\ntest_df = load_test_data(os.path.join(data_dir, 'text', 'test2.txt'))\n\n# ğŸ§ª Apply testing mode if enabled\nif TESTING_MODE:\n    print(f\"\\nğŸ§ª TESTING MODE ENABLED\")\n    print(f\"Limiting data to {TEST_SAMPLE_SIZE} samples for testing\")\n    \n    # Take first TEST_SAMPLE_SIZE samples for quick testing\n    original_size = len(test_df)\n    test_df = test_df.head(TEST_SAMPLE_SIZE).copy()\n    \n    print(f\"Data reduced: {original_size} â†’ {len(test_df)} samples\")\n    print(\"âš ï¸  Remember to set TESTING_MODE = False for full processing\")\nelse:\n    print(f\"\\nğŸš€ FULL PROCESSING MODE\")\n    print(f\"Processing all {len(test_df)} samples\")\n\n# Add image paths\nimage_folder = os.path.join(data_dir, 'dataset_image')\ntest_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(image_folder, f\"{x}.jpg\"))\n\n# Clean data\ninitial_count = len(test_df)\ntest_df.dropna(subset=['text'], inplace=True)\ntest_df['sarcasm'] = test_df['sarcasm'].astype(int)\ntest_df.drop(test_df[~test_df['image_path'].apply(os.path.exists)].index, inplace=True)\nfinal_count = len(test_df)\n\nprint(f\"\\nData cleaned: {initial_count} â†’ {final_count} samples\")\nprint(f\"Sarcastic: {len(test_df[test_df['sarcasm']==1])}\")\nprint(f\"Non-sarcastic: {len(test_df[test_df['sarcasm']==0])}\")\n\n# Show estimated time based on current data size\nestimated_time_minutes = (final_count * 8) / 60\nif estimated_time_minutes < 60:\n    print(f\"â±ï¸  Estimated processing time: {estimated_time_minutes:.1f} minutes\")\nelse:\n    print(f\"â±ï¸  Estimated processing time: {estimated_time_minutes/60:.1f} hours\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:17.072437Z","iopub.execute_input":"2025-06-19T15:37:17.072772Z","iopub.status.idle":"2025-06-19T15:37:17.197312Z","shell.execute_reply.started":"2025-06-19T15:37:17.072746Z","shell.execute_reply":"2025-06-19T15:37:17.196475Z"}},"outputs":[{"name":"stdout","text":"Loading test data...\nâœ“ Dataset found at: /kaggle/input/data-of-multimodal-sarcasm-detection\nLoading /kaggle/input/data-of-multimodal-sarcasm-detection/text/test2.txt...\nâœ“ Loaded 2409 test records\n\nğŸ§ª TESTING MODE ENABLED\nLimiting data to 20 samples for testing\nData reduced: 2409 â†’ 20 samples\nâš ï¸  Remember to set TESTING_MODE = False for full processing\n\nData cleaned: 20 â†’ 20 samples\nSarcastic: 20\nNon-sarcastic: 0\nâ±ï¸  Estimated processing time: 2.7 minutes\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport time\nimport pandas as pd\n\n# --- CONFIGURATION ---\nTESTING_MODE = True  # Set to True for testing mode\n\n# --- LLM PREPROCESSING FUNCTION ---\ndef preprocess_with_llm(text):\n    \"\"\"Preprocess text with LLM\"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return \"\"\n        \n    try:\n        completion = openrouter_client.chat.completions.create(\n            model=\"mistralai/mistral-nemo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert text preprocessor for a machine learning model. Your task is to clean and standardize tweet text. Follow these rules strictly:\\n1. Correct typos and grammatical errors.\\n2. Expand internet slang and abbreviations into standard English (e.g., 'lol' becomes 'laughing out loud').\\n3. Convert hashtags into meaningful phrases (e.g., '#nosleep' becomes 'no sleep').\\n4. Remove any URLs and mentions like '<user>'.\\n5. CRITICALLY IMPORTANT: Preserve the original tone, especially sarcasm or irony. Do not change the underlying meaning.\\n6. Your output must ONLY be the final cleaned text, with no extra explanations or chat.\"},\n                {\"role\": \"user\", \"content\": f\"Please preprocess the following tweet: \\\"<user> OMG u kno what i mean?! today is going to be awesome! #nosleep #bestdayever\\\"\"},\n                {\"role\": \"assistant\", \"content\": \"Oh my god, you know what I mean?! Today is going to be awesome! No sleep. Best day ever.\"},\n                {\"role\": \"user\", \"content\": f\"Please preprocess the following tweet: \\\"{text}\\\"\"}\n            ],\n            temperature=0.1,\n            max_tokens=150,\n        )\n        cleaned_text = completion.choices[0].message.content.strip()\n        time.sleep(8)  # Rate limiting - 8 seconds between requests\n        return cleaned_text\n    except Exception as e:\n        if \"429\" in str(e):  # Rate limit\n            print(f\"Rate limit hit, waiting 60 seconds...\")\n            time.sleep(60)\n            return text  # Return original on rate limit\n        else:\n            print(f\"Error processing text: {e}\")\n            return text  # Return original on other errors\n\nprint(\"âœ“ LLM preprocessing function ready\")\n\n# --- OUTPUT FILE SETUP ---\n\n# Set output filenames based on mode\nif TESTING_MODE:\n    OUTPUT_CSV = \"test_preprocessed_testing.csv\"\n    print(f\"ğŸ§ª Testing mode: Output will be saved to '{OUTPUT_CSV}'\")\nelse:\n    OUTPUT_CSV = \"test_preprocessed.csv\"\n    \n    # ğŸ”„ SMART RESUME: Check if testing file exists and merge it\n    testing_csv = \"test_preprocessed_testing.csv\"\n    if os.path.exists(testing_csv):\n        print(f\"ğŸ”„ Found testing results in '{testing_csv}'\")\n        print(\"Smart resume: Will include testing data in full processing\")\n        \n        # Load existing testing data to avoid reprocessing\n        existing_df = pd.read_csv(testing_csv)\n        print(f\"Found {len(existing_df)} preprocessed testing samples\")\n        \n        # Mark testing IDs to skip\n        testing_ids = set(existing_df['id'].astype(str))\n        print(f\"Will skip {len(testing_ids)} already processed samples\")\n    else:\n        testing_ids = set()\n        print(\"No testing data found - processing from scratch\")\n    \n    print(f\"ğŸš€ Full mode: Final output will be saved to '{OUTPUT_CSV}'\")\n\nprint(f\"\\nOutput file: {OUTPUT_CSV}\")\n\n# Resume capability\nresume_data = []\nprocessed_ids = set()\n\nif os.path.exists(OUTPUT_CSV):\n    print(f\"ğŸ“„ Found existing output file: {OUTPUT_CSV}\")\n    try:\n        existing_df = pd.read_csv(OUTPUT_CSV)\n        resume_data = existing_df.to_dict('records')\n        processed_ids = set(existing_df['id'].astype(str))\n        print(f\"âœ“ Loaded {len(resume_data)} previously processed samples\")\n        print(f\"Remaining to process: {len(test_df) - len(processed_ids)}\")\n    except Exception as e:\n        print(f\"âš ï¸  Error loading existing file: {e}\")\n        print(\"Starting fresh...\")\n        resume_data = []\n        processed_ids = set()\nelse:\n    print(f\"ğŸ“ Creating new output file: {OUTPUT_CSV}\")\n\n# For full mode with smart resume, also skip testing IDs\nif not TESTING_MODE and testing_ids:\n    # Add testing data to resume data if not already there\n    if testing_csv and os.path.exists(testing_csv):\n        testing_df = pd.read_csv(testing_csv)\n        for _, row in testing_df.iterrows():\n            if str(row['id']) not in processed_ids:\n                resume_data.append(row.to_dict())\n                processed_ids.add(str(row['id']))\n        print(f\"Smart resume: Added {len(testing_df)} testing samples to skip list\")\n\nprint(f\"Total samples to skip: {len(processed_ids)}\")\nprint(f\"Samples remaining to process: {len(test_df) - len(processed_ids)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:17.198300Z","iopub.execute_input":"2025-06-19T15:37:17.198580Z","iopub.status.idle":"2025-06-19T15:37:17.212811Z","shell.execute_reply.started":"2025-06-19T15:37:17.198559Z","shell.execute_reply":"2025-06-19T15:37:17.211900Z"}},"outputs":[{"name":"stdout","text":"âœ“ LLM preprocessing function ready\nğŸ§ª Testing mode: Output will be saved to 'test_preprocessed_testing.csv'\n\nOutput file: test_preprocessed_testing.csv\nğŸ“ Creating new output file: test_preprocessed_testing.csv\nTotal samples to skip: 0\nSamples remaining to process: 20\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- BATCH PROCESSING WITH RESUME CAPABILITY ---\noutput_file = \"/kaggle/working/test_processed_complete.csv\"\nsave_frequency = 20  # Save progress every 25 samples\n\n# --- PROCESSING CONFIGURATION ---\n\n# ğŸ“Š Batch processing settings\nif TESTING_MODE:\n    BATCH_SIZE = 10          # Small batches for testing\n    SAVE_FREQUENCY = 10      # Save every 5 samples in testing mode\nelse:\n    BATCH_SIZE = 20         # Larger batches for efficiency in full mode\n    SAVE_FREQUENCY = 20     # Save every 50 samples in full mode\n\nprint(f\"âš™ï¸  Batch size: {BATCH_SIZE}\")\nprint(f\"ğŸ’¾ Save frequency: every {SAVE_FREQUENCY} samples\")\nprint(f\"ğŸ“Š Total samples to process: {len(test_df) - len(processed_ids)}\")\n\n# Check for existing progress\nstart_idx = 0\nprocessed_data = []\n\nif os.path.exists(output_file):\n    try:\n        existing_df = pd.read_csv(output_file)\n        if 'processed_text' in existing_df.columns:\n            processed_data = existing_df.to_dict('records')\n            start_idx = len(processed_data)\n            print(f\"âœ“ Resuming from index {start_idx} ({start_idx}/{len(test_df)} completed)\")\n        else:\n            print(\"Existing file found but invalid format, starting fresh\")\n    except Exception as e:\n        print(f\"Error reading existing file: {e}, starting fresh\")\n\nif start_idx == 0:\n    print(\"Starting fresh preprocessing...\")\n\n# Calculate remaining work\nremaining_samples = len(test_df) - start_idx\nestimated_time_minutes = (remaining_samples * 10) / 60  # 8 seconds per sample\nprint(f\"Remaining samples: {remaining_samples}\")\nprint(f\"Estimated time: {estimated_time_minutes:.1f} minutes\")\n\nif remaining_samples == 0:\n    print(\"âœ“ All data already processed!\")\nelse:\n    # Process remaining data\n    total_processed = len(processed_data)\n    start_time = time.time()\n    \n    for i in tqdm(range(start_idx, len(test_df)), desc=\"Processing test data\"):\n        row = test_df.iloc[i]\n        \n        if str(row['id']) in processed_ids:\n            continue\n        \n        # Create processed row\n        processed_row = {\n            'id': row['id'],\n            'text': row['text'],\n            'sarcasm': row['sarcasm'],\n            'image_path': row['image_path']\n        }\n        \n        # Process with LLM\n        try:\n            processed_text = preprocess_with_llm(row['text'])\n            processed_row['processed_text'] = processed_text\n        except Exception as e:\n            print(f\"Failed to process sample {i}: {e}\")\n            processed_row['processed_text'] = row['text']  # Use original\n        \n        processed_data.append(processed_row)\n        total_processed += 1\n        \n        # Progress update\n        if total_processed % 10 == 0:\n            elapsed = time.time() - start_time\n            rate = total_processed / elapsed if elapsed > 0 else 0\n            remaining = len(test_df) - len(processed_ids) - (total_processed - len(processed_ids))\n            eta = remaining / rate if rate > 0 else 0\n            print(f\"âœ“ Processed {total_processed}/{len(test_df)} | Rate: {rate:.1f}/min | ETA: {eta/60:.1f}min\")\n        \n        # Save intermediate results\n        if total_processed % SAVE_FREQUENCY == 0:\n            temp_df = pd.DataFrame(processed_data)\n            temp_df.to_csv(output_file, index=False)\n            print(f\"ğŸ’¾ Saved {len(temp_df)} samples to {output_file}\")\n            print(f\"â±ï¸  Estimated remaining: {((len(test_df) - len(processed_data)) * 8) / 60:.1f} minutes\")\n\n# Final save\nfinal_df = pd.DataFrame(processed_data)\nfinal_df.to_csv(output_file, index=False)\n\nelapsed_time = time.time() - start_time\nprint(f\"\\nğŸ‰ TEST DATA PREPROCESSING COMPLETED!\")\nprint(f\"ğŸ“„ Output file: {output_file}\")\nprint(f\"ğŸ“Š Total samples processed: {len(final_df)}\")\nprint(f\"ğŸ’¾ File size: {os.path.getsize(output_file) / (1024*1024):.1f} MB\")\nprint(f\"â° Total time: {elapsed_time/60:.1f} minutes\")\nprint(f\"ğŸš€ Average rate: {len(final_df)/(elapsed_time/60):.1f} samples/minute\")\nprint(\"\\nâœ… Ready for download and use in training notebook!\")\n\n# Display sample results\nprint(f\"\\nğŸ“‹ Sample of processed data:\")\nprint(final_df[['id', 'sarcasm', 'text', 'processed_text']].head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:17.213787Z","iopub.execute_input":"2025-06-19T15:37:17.214121Z","iopub.status.idle":"2025-06-19T15:40:35.758217Z","shell.execute_reply.started":"2025-06-19T15:37:17.214100Z","shell.execute_reply":"2025-06-19T15:40:35.757235Z"}},"outputs":[{"name":"stdout","text":"âš™ï¸  Batch size: 10\nğŸ’¾ Save frequency: every 10 samples\nğŸ“Š Total samples to process: 20\nStarting fresh preprocessing...\nRemaining samples: 20\nEstimated time: 3.3 minutes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing test data:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e15a3ede6fa24c1a927a4bd90c01833a"}},"metadata":{}},{"name":"stdout","text":"âœ“ Processed 10/20 | Rate: 0.1/min | ETA: 1.6min\nğŸ’¾ Saved 10 samples to /kaggle/working/test_processed_complete.csv\nâ±ï¸  Estimated remaining: 1.3 minutes\nâœ“ Processed 20/20 | Rate: 0.1/min | ETA: 0.0min\nğŸ’¾ Saved 20 samples to /kaggle/working/test_processed_complete.csv\nâ±ï¸  Estimated remaining: 0.0 minutes\n\nğŸ‰ TEST DATA PREPROCESSING COMPLETED!\nğŸ“„ Output file: /kaggle/working/test_processed_complete.csv\nğŸ“Š Total samples processed: 20\nğŸ’¾ File size: 0.0 MB\nâ° Total time: 3.3 minutes\nğŸš€ Average rate: 6.0 samples/minute\n\nâœ… Ready for download and use in training notebook!\n\nğŸ“‹ Sample of processed data:\n                   id  sarcasm  \\\n0  862902619928506372        1   \n1  892551658487631873        1   \n2  853143461360480256        1   \n3  918423568823840768        1   \n4  731617467718610944        1   \n\n                                                text  \\\n0  i am guessing # netflix no longer lets you gra...   \n1  it 's the insensitive strikeouts at suntrust p...   \n2  following the path of the river calder , so .....   \n3  # westernsahara # authority has no lessons 2ge...   \n4                           hey <user> great sale !    \n\n                                      processed_text  \n0  I'm guessing Netflix no longer allows you to t...  \n1  It's the insensitive strikeouts at SunTrust Pa...  \n2  Following the path of the River Calder, so gri...  \n3  Western Sahara authority has no lessons to get...  \n4                                   Hey, great sale!  \n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- VERIFICATION & SUMMARY ---\nprint(\"=== FINAL VERIFICATION ===\")\n\n# Load and verify final file\nfinal_df = pd.read_csv(output_file)\n\nprint(f\"ğŸ“Š Dataset Summary:\")\nprint(f\"   Total samples: {len(final_df)}\")\nprint(f\"   Sarcastic: {len(final_df[final_df['sarcasm']==1])}\")\nprint(f\"   Non-sarcastic: {len(final_df[final_df['sarcasm']==0])}\")\nprint(f\"   Missing processed_text: {final_df['processed_text'].isna().sum()}\")\n\n# Show sample of processed data\nprint(f\"\\nğŸ“ Sample of processed data:\")\nfor i in range(min(3, len(final_df))):\n    row = final_df.iloc[i]\n    print(f\"\\nSample {i+1}:\")\n    print(f\"  Original:  {row['text'][:100]}...\")\n    print(f\"  Processed: {row['processed_text'][:100]}...\")\n    print(f\"  Label: {row['sarcasm']}\")\n\nprint(f\"\\nğŸ¯ MISSION ACCOMPLISHED - PERSON 3 TASK COMPLETE!\")\nprint(f\"ğŸ“¤ Download the file and share with team for training phase\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:40:35.759136Z","iopub.execute_input":"2025-06-19T15:40:35.759444Z","iopub.status.idle":"2025-06-19T15:40:35.774107Z","shell.execute_reply.started":"2025-06-19T15:40:35.759418Z","shell.execute_reply":"2025-06-19T15:40:35.773260Z"}},"outputs":[{"name":"stdout","text":"=== FINAL VERIFICATION ===\nğŸ“Š Dataset Summary:\n   Total samples: 20\n   Sarcastic: 20\n   Non-sarcastic: 0\n   Missing processed_text: 0\n\nğŸ“ Sample of processed data:\n\nSample 1:\n  Original:  i am guessing # netflix no longer lets you grab screens of movies . that & the new rating system is ...\n  Processed: I'm guessing Netflix no longer allows you to take screenshots of movies. That, and the new rating sy...\n  Label: 1\n\nSample 2:\n  Original:  it 's the insensitive strikeouts at suntrust park .  # braves # chopchop...\n  Processed: It's the insensitive strikeouts at SunTrust Park. Braves. Chop chop....\n  Label: 1\n\nSample 3:\n  Original:  following the path of the river calder , so ... grim ... up .... north ...\n  Processed: Following the path of the River Calder, so grim up north...\n  Label: 1\n\nğŸ¯ MISSION ACCOMPLISHED - PERSON 3 TASK COMPLETE!\nğŸ“¤ Download the file and share with team for training phase\n","output_type":"stream"}],"execution_count":6}]}