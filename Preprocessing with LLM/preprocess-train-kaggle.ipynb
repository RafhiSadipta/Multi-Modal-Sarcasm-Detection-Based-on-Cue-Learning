{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":9560502,"datasetId":5826041,"databundleVersionId":9778378},{"sourceType":"datasetVersion","sourceId":12196461,"datasetId":7682435,"databundleVersionId":12735515}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- ENVIRONMENT & LIBRARY SETUP ---\nimport os\nimport sys\nimport pandas as pd\nimport time\nfrom tqdm.notebook import tqdm\nimport ast\n\nprint(\"=== TRAIN DATA PREPROCESSING SETUP ===\")\nprint(f\"Python version: {sys.version.split()[0]}\")\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Validate Kaggle environment\nassert '/kaggle/' in os.getcwd(), \"This notebook must run in Kaggle environment!\"\nprint(\"‚úì Confirmed running in Kaggle environment\")\n\n# Install required packages\nprint(\"Installing OpenAI package...\")\n!pip install -q openai\n\nfrom openai import OpenAI\nprint(\"‚úì Libraries imported successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:11.557301Z","iopub.execute_input":"2025-06-19T15:37:11.557524Z","iopub.status.idle":"2025-06-19T15:37:19.515517Z","shell.execute_reply.started":"2025-06-19T15:37:11.557505Z","shell.execute_reply":"2025-06-19T15:37:19.514507Z"}},"outputs":[{"name":"stdout","text":"=== TRAIN DATA PREPROCESSING SETUP ===\nPython version: 3.11.11\nWorking directory: /kaggle/working\n‚úì Confirmed running in Kaggle environment\nInstalling OpenAI package...\n‚úì Libraries imported successfully\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- API SETUP ---\nprint(\"Setting up OpenRouter API...\")\n\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    api_key = user_secrets.get_secret(\"OPENROUTER_API_KEY\")\n    if api_key:\n        print(\"‚úì OpenRouter API key loaded from Kaggle Secrets\")\n    else:\n        raise ValueError(\"No API key found\")\nexcept Exception as e:\n    print(f\"‚ùå ERROR: Could not load API key: {e}\")\n    print(\"Please add OPENROUTER_API_KEY to Kaggle Secrets\")\n    raise\n\n# Initialize OpenRouter client\nopenrouter_client = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=api_key,\n)\n\nprint(\"‚úì OpenRouter client initialized\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:19.516418Z","iopub.execute_input":"2025-06-19T15:37:19.516827Z","iopub.status.idle":"2025-06-19T15:37:20.288716Z","shell.execute_reply.started":"2025-06-19T15:37:19.516798Z","shell.execute_reply":"2025-06-19T15:37:20.287711Z"}},"outputs":[{"name":"stdout","text":"Setting up OpenRouter API...\n‚úì OpenRouter API key loaded from Kaggle Secrets\n‚úì OpenRouter client initialized\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- DATA LOADING ---\nprint(\"Loading training data...\")\n\n# üß™ TESTING MODE - Set to True for small scale testing\nTESTING_MODE = True  # Change to False for full processing\nTEST_SAMPLE_SIZE = 20  # Number of samples for testing\n\n# üìä BATCH PROCESSING CONFIGURATION\n# Set these parameters to process specific ranges of data\nENABLE_RANGE_PROCESSING = False  # Set to True to enable range processing\nSTART_ROW = 28000       # Start from this row (0-based index)\nEND_ROW = -1      # End at this row (exclusive, so this processes rows 0-3499)\n\n# Find dataset\nkaggle_data_paths = [\n    \"/kaggle/input/data-of-multimodal-sarcasm-detection\",\n]\n\ndata_dir = None\nfor path in kaggle_data_paths:\n    if os.path.exists(path):\n        data_dir = path\n        print(f\"‚úì Dataset found at: {data_dir}\")\n        break\n\nif data_dir is None:\n    print(\"‚ùå ERROR: Dataset not found!\")\n    print(\"Please add the multimodal sarcasm detection dataset to this notebook\")\n    raise FileNotFoundError(\"Dataset not found\")\n\ndef load_train_data(filepath):\n    \"\"\"Load training data efficiently\"\"\"\n    records = []\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"Training file not found: {filepath}\")\n        \n    print(f\"Loading {filepath}...\")\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line_num, line in enumerate(f, 1):\n            try:\n                data_list = ast.literal_eval(line.strip())\n                if len(data_list) >= 3:\n                    records.append({\n                        'id': data_list[0], \n                        'text': data_list[1], \n                        'sarcasm': int(data_list[2])\n                    })\n            except (ValueError, SyntaxError, IndexError):\n                if line_num % 1000 == 0:  # Log every 1000 errors\n                    print(f\"Skipped malformed line {line_num}\")\n                continue\n    \n    print(f\"‚úì Loaded {len(records)} training records\")\n    return pd.DataFrame(records)\n\n# Load training data\ntrain_df = load_train_data(os.path.join(data_dir, 'text', 'train.txt'))\n\n# üß™ Apply testing mode if enabled\nif TESTING_MODE:\n    print(f\"\\nüß™ TESTING MODE ENABLED\")\n    print(f\"Limiting data to {TEST_SAMPLE_SIZE} samples for testing\")\n    \n    # Take first TEST_SAMPLE_SIZE samples for quick testing\n    original_size = len(train_df)\n    train_df = train_df.head(TEST_SAMPLE_SIZE).copy()\n    \n    print(f\"Data reduced: {original_size} ‚Üí {len(train_df)} samples\")\n    print(\"‚ö†Ô∏è  Remember to set TESTING_MODE = False for full processing\")\nelif ENABLE_RANGE_PROCESSING:\n    print(f\"\\nüìä RANGE PROCESSING MODE ENABLED\")\n    original_size = len(train_df)\n    \n    # Handle negative END_ROW (means process till end)\n    actual_end_row = len(train_df) if END_ROW == -1 else min(END_ROW, len(train_df))\n    actual_start_row = max(0, START_ROW)\n    \n    print(f\"Processing rows {actual_start_row} to {actual_end_row-1}\")\n    print(f\"Total samples in this batch: {actual_end_row - actual_start_row}\")\n    \n    # Slice the dataframe to specified range\n    train_df = train_df.iloc[actual_start_row:actual_end_row].copy()\n    train_df.reset_index(drop=True, inplace=True)\n    \n    print(f\"Data filtered: {original_size} ‚Üí {len(train_df)} samples\")\n    print(f\"üìÅ Output will include batch info: batch_{actual_start_row}_{actual_end_row}\")\nelse:\n    print(f\"\\nüöÄ FULL PROCESSING MODE\")\n    print(f\"Processing all {len(train_df)} samples\")\n    print(\"‚ö†Ô∏è  This will take 3-4 hours. Consider using RANGE PROCESSING for large datasets.\")\n\n# Add image paths\nimage_folder = os.path.join(data_dir, 'dataset_image')\ntrain_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(image_folder, f\"{x}.jpg\"))\n\n# Clean data\ninitial_count = len(train_df)\nprint(f\"\\nüßπ CLEANING DATA:\")\nprint(f\"Initial count: {initial_count}\")\n\n# Check for missing text\ntext_before = len(train_df)\ntrain_df.dropna(subset=['text'], inplace=True)\ntext_after = len(train_df)\ntext_dropped = text_before - text_after\nprint(f\"Dropped {text_dropped} rows with missing text ({text_dropped/text_before*100:.1f}%)\")\n\n# Convert sarcasm to int\ntrain_df['sarcasm'] = train_df['sarcasm'].astype(int)\n\n# Check for missing images\nimage_before = len(train_df)\nmissing_images = []\nfor idx, row in train_df.iterrows():\n    if not os.path.exists(row['image_path']):\n        missing_images.append(row['id'])\n\nif missing_images:\n    print(f\"Found {len(missing_images)} missing image files\")\n    print(f\"Sample missing images: {missing_images[:5]}...\")\n    \ntrain_df.drop(train_df[~train_df['image_path'].apply(os.path.exists)].index, inplace=True)\nimage_after = len(train_df)\nimage_dropped = image_before - image_after\nprint(f\"Dropped {image_dropped} rows with missing images ({image_dropped/image_before*100:.1f}%)\")\n\nfinal_count = len(train_df)\n\nprint(f\"\\nData cleaned: {initial_count} ‚Üí {final_count} samples\")\nprint(f\"Sarcastic: {len(train_df[train_df['sarcasm']==1])}\")\nprint(f\"Non-sarcastic: {len(train_df[train_df['sarcasm']==0])}\")\n\n# Show estimated time based on current data size\nestimated_time_minutes = (final_count * 10) / 60\nif estimated_time_minutes < 60:\n    print(f\"‚è±Ô∏è  Estimated processing time: {estimated_time_minutes:.1f} minutes\")\nelse:\n    print(f\"‚è±Ô∏è  Estimated processing time: {estimated_time_minutes/60:.1f} hours\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:20.290614Z","iopub.execute_input":"2025-06-19T15:37:20.290941Z","iopub.status.idle":"2025-06-19T15:37:20.800733Z","shell.execute_reply.started":"2025-06-19T15:37:20.290917Z","shell.execute_reply":"2025-06-19T15:37:20.799808Z"}},"outputs":[{"name":"stdout","text":"Loading training data...\n‚úì Dataset found at: /kaggle/input/data-of-multimodal-sarcasm-detection\nLoading /kaggle/input/data-of-multimodal-sarcasm-detection/text/train.txt...\n‚úì Loaded 29040 training records\n\nüß™ TESTING MODE ENABLED\nLimiting data to 20 samples for testing\nData reduced: 29040 ‚Üí 20 samples\n‚ö†Ô∏è  Remember to set TESTING_MODE = False for full processing\n\nüßπ CLEANING DATA:\nInitial count: 20\nDropped 0 rows with missing text (0.0%)\nFound 5 missing image files\nSample missing images: ['910308516510011393', '725333760762363905', '854334602516733952', '904880359350964224', '935145326553456641']...\nDropped 5 rows with missing images (25.0%)\n\nData cleaned: 20 ‚Üí 15 samples\nSarcastic: 15\nNon-sarcastic: 0\n‚è±Ô∏è  Estimated processing time: 2.5 minutes\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- LLM PREPROCESSING FUNCTION ---\ndef preprocess_with_llm(text):\n    \"\"\"Preprocess text with LLM\"\"\"\n    if not isinstance(text, str) or not text.strip():\n        return \"\"\n        \n    try:\n        completion = openrouter_client.chat.completions.create(\n            model=\"mistralai/mistral-nemo\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert text preprocessor for a machine learning model. Your task is to clean and standardize tweet text. Follow these rules strictly:\\n1. Correct typos and grammatical errors.\\n2. Expand internet slang and abbreviations into standard English (e.g., 'lol' becomes 'laughing out loud').\\n3. Convert hashtags into meaningful phrases (e.g., '#nosleep' becomes 'no sleep').\\n4. Remove any URLs and mentions like '<user>'.\\n5. CRITICALLY IMPORTANT: Preserve the original tone, especially sarcasm or irony. Do not change the underlying meaning.\\n6. Your output must ONLY be the final cleaned text, with no extra explanations or chat.\"},\n                {\"role\": \"user\", \"content\": f\"Please preprocess the following tweet: \\\"<user> OMG u kno what i mean?! today is going to be awesome! #nosleep #bestdayever\\\"\"},\n                {\"role\": \"assistant\", \"content\": \"Oh my god, you know what I mean?! Today is going to be awesome! No sleep. Best day ever.\"},\n                {\"role\": \"user\", \"content\": f\"Please preprocess the following tweet: \\\"{text}\\\"\"}\n            ],\n            temperature=0.1,\n            max_tokens=150,\n        )\n        cleaned_text = completion.choices[0].message.content.strip()\n        time.sleep(6)  # Rate limiting - 8 seconds between requests\n        return cleaned_text\n    except Exception as e:\n        if \"429\" in str(e):  # Rate limit\n            print(f\"Rate limit hit, waiting 60 seconds...\")\n            time.sleep(60)\n            return text  # Return original on rate limit\n        else:\n            print(f\"Error processing text: {e}\")\n            return text  # Return original on other errors\n\nprint(\"‚úì LLM preprocessing function ready\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:20.801870Z","iopub.execute_input":"2025-06-19T15:37:20.802242Z","iopub.status.idle":"2025-06-19T15:37:20.811204Z","shell.execute_reply.started":"2025-06-19T15:37:20.802180Z","shell.execute_reply":"2025-06-19T15:37:20.810101Z"}},"outputs":[{"name":"stdout","text":"‚úì LLM preprocessing function ready\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- BATCH PROCESSING WITH RESUME CAPABILITY ---\n\n# Set output filename and save frequency based on mode\nif TESTING_MODE:\n    output_file = \"/kaggle/working/train_processed_TEST.csv\"\n    save_frequency = 10  # Save every 5 samples for testing\n    print(f\"üß™ Testing mode: Output will be saved as train_processed_TEST.csv\")\nelif ENABLE_RANGE_PROCESSING:\n    # Create filename with batch info\n    actual_end_row = len(train_df) + START_ROW if END_ROW == -1 else END_ROW\n    batch_name = f\"batch_{START_ROW}_{actual_end_row}\"\n    output_file = f\"/kaggle/working/train_processed_{batch_name}.csv\"\n    save_frequency = 50  # Save every x samples for batch processing\n    print(f\"üìä Range mode: Output will be saved as train_processed_{batch_name}.csv\")\n    print(f\"üìÅ Processing samples {START_ROW} to {actual_end_row-1}\")\nelse:\n    output_file = \"/kaggle/working/train_processed_complete.csv\"\n    save_frequency = 50  # Save every 50 samples for full processing\n    print(f\"üöÄ Full mode: Output will be saved as train_processed_complete.csv\")\n\nbatch_size = 100  # Not currently used, but kept for reference\n\n# Check for existing progress\nstart_idx = 0\nprocessed_data = []\n\n# For full mode, check if we can resume from testing mode results\nif not TESTING_MODE:\n    test_file = \"/kaggle/working/train_processed_TEST.csv\"\n    if os.path.exists(test_file) and not os.path.exists(output_file):\n        try:\n            test_df = pd.read_csv(test_file)\n            if 'processed_text' in test_df.columns and len(test_df) > 0:\n                print(f\"üîÑ Found testing results ({len(test_df)} samples)\")\n                print(f\"üìã These will be included in full processing to avoid reprocessing\")\n                processed_data = test_df.to_dict('records')\n                start_idx = len(processed_data)\n        except Exception as e:\n            print(f\"Could not load test results: {e}\")\n\n# Check for existing progress in main output file\nif os.path.exists(output_file):\n    try:\n        existing_df = pd.read_csv(output_file)\n        if 'processed_text' in existing_df.columns:\n            # If we don't have any data yet, load from existing file\n            if len(processed_data) == 0:\n                processed_data = existing_df.to_dict('records')\n                start_idx = len(processed_data)\n            # If existing file has more data than our current progress, use it\n            elif len(existing_df) > len(processed_data):\n                processed_data = existing_df.to_dict('records')\n                start_idx = len(processed_data)\n            \n            print(f\"‚úì Resuming from index {start_idx} ({start_idx}/{len(train_df)} completed)\")\n        else:\n            print(\"Existing file found but invalid format, starting fresh\")\n    except Exception as e:\n        print(f\"Error reading existing file: {e}, starting fresh\")\n\nif start_idx == 0:\n    print(\"Starting fresh preprocessing...\")\nelif start_idx > 0 and not TESTING_MODE:\n    print(f\"üìà Will continue processing from sample {start_idx+1} to {len(train_df)}\")\n\n# Calculate remaining work\nremaining_samples = len(train_df) - start_idx\nestimated_time_minutes = (remaining_samples * 10) / 60  # 8 seconds per sample\nprint(f\"Remaining samples: {remaining_samples}\")\n\nif estimated_time_minutes < 60:\n    print(f\"Estimated time: {estimated_time_minutes:.1f} minutes\")\nelse:\n    print(f\"Estimated time: {estimated_time_minutes/60:.1f} hours\")\n\nif remaining_samples == 0:\n    print(\"‚úì All data already processed!\")\nelse:\n    # Process remaining data\n    total_processed = len(processed_data)\n    start_time = time.time()\n    \n    for i in tqdm(range(start_idx, len(train_df)), desc=\"Processing training data\"):\n        row = train_df.iloc[i]\n        \n        # Create processed row\n        processed_row = {\n            'id': row['id'],\n            'text': row['text'],\n            'sarcasm': row['sarcasm'],\n            'image_path': row['image_path']\n        }\n        \n        # Process with LLM\n        try:\n            processed_text = preprocess_with_llm(row['text'])\n            processed_row['processed_text'] = processed_text\n        except Exception as e:\n            print(f\"Failed to process sample {i}: {e}\")\n            processed_row['processed_text'] = row['text']  # Use original\n        \n        processed_data.append(processed_row)\n        total_processed += 1\n        \n        # Progress update - UBAH ANGKA INI UNTUK MENGATUR FREKUENSI PROGRESS\n        if total_processed % 10 == 0:  # ‚Üê Ganti 10 dengan angka lain (50, 100, dll)\n            elapsed = time.time() - start_time\n            rate = total_processed / elapsed if elapsed > 0 else 0\n            remaining = len(train_df) - len(processed_data)\n            eta = remaining / rate if rate > 0 else 0\n            print(f\"‚úì Processed {total_processed}/{len(train_df)} | Rate: {rate:.1f}/min | ETA: {eta/60:.1f}min\")\n        \n        # Save progress periodically\n        if (i + 1) % save_frequency == 0:\n            temp_df = pd.DataFrame(processed_data)\n            temp_df.to_csv(output_file, index=False)\n            print(f\"\\nüíæ Progress saved: {len(processed_data)}/{len(train_df)} samples\")\n            \n            remaining_after_save = len(train_df) - len(processed_data)\n            time_remaining_minutes = (remaining_after_save * 10) / 60\n            \n            if time_remaining_minutes < 60:\n                print(f\"‚è±Ô∏è  Estimated remaining: {time_remaining_minutes:.1f} minutes\")\n            else:\n                print(f\"‚è±Ô∏è  Estimated remaining: {time_remaining_minutes/60:.1f} hours\")\n\n# Final save\nfinal_df = pd.DataFrame(processed_data)\nfinal_df.to_csv(output_file, index=False)\n\nelapsed_time = time.time() - start_time\nprint(f\"\\nüéâ PREPROCESSING COMPLETED!\")\nprint(f\"üìÑ Output file: {output_file}\")\nprint(f\"üìä Total samples processed: {len(final_df)}\")\nprint(f\"üíæ File size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\nprint(f\"‚è∞ Total time: {elapsed_time/60:.1f} minutes\")\nprint(f\"üöÄ Average rate: {len(final_df)/(elapsed_time/60):.1f} samples/minute\")\n\nif TESTING_MODE:\n    print(f\"\\nüß™ TESTING COMPLETE!\")\n    print(f\"‚úÖ CSV file created successfully with {len(final_df)} samples\")\n    print(f\"üîÑ To process full data: Set TESTING_MODE = False in cell 4\")\n    print(f\"üìã Your test results will be automatically included in full processing!\")\nelif ENABLE_RANGE_PROCESSING:\n    actual_end_row = len(final_df) + START_ROW\n    print(f\"\\nüìä BATCH PROCESSING COMPLETE!\")\n    print(f\"‚úÖ Batch {START_ROW}-{actual_end_row-1} processed successfully\")\n    print(f\"üìÅ File: train_processed_batch_{START_ROW}_{actual_end_row}.csv\")\n    print(f\"üîÑ To process next batch: Update START_ROW and END_ROW in cell 4\")\n    print(f\"üìã After all batches: Use merge script to combine all batch files\")\n    print(f\"\\nüí° Next batch suggestion:\")\n    print(f\"   START_ROW = {actual_end_row}\")\n    print(f\"   END_ROW = {actual_end_row + 5000}  # or -1 for remaining data\")\nelse:\n    print(f\"\\n‚úÖ Ready for download and use in training notebook!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:37:20.815275Z","iopub.execute_input":"2025-06-19T15:37:20.815692Z","iopub.status.idle":"2025-06-19T15:39:21.702368Z","shell.execute_reply.started":"2025-06-19T15:37:20.815667Z","shell.execute_reply":"2025-06-19T15:39:21.701321Z"}},"outputs":[{"name":"stdout","text":"üß™ Testing mode: Output will be saved as train_processed_TEST.csv\nStarting fresh preprocessing...\nRemaining samples: 15\nEstimated time: 2.5 minutes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing training data:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"377181ffd35d477082ed7154556f9625"}},"metadata":{}},{"name":"stdout","text":"‚úì Processed 10/15 | Rate: 0.1/min | ETA: 0.7min\n\nüíæ Progress saved: 10/15 samples\n‚è±Ô∏è  Estimated remaining: 0.8 minutes\n\nüéâ PREPROCESSING COMPLETED!\nüìÑ Output file: /kaggle/working/train_processed_TEST.csv\nüìä Total samples processed: 15\nüíæ File size: 0.00 MB\n‚è∞ Total time: 2.0 minutes\nüöÄ Average rate: 7.4 samples/minute\n\nüß™ TESTING COMPLETE!\n‚úÖ CSV file created successfully with 15 samples\nüîÑ To process full data: Set TESTING_MODE = False in cell 4\nüìã Your test results will be automatically included in full processing!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# --- VERIFICATION & SUMMARY ---\nprint(\"=== FINAL VERIFICATION ===\")\n\n# Load and verify final file\nfinal_df = pd.read_csv(output_file)\n\nprint(f\"üìä Dataset Summary:\")\nprint(f\"   Total samples: {len(final_df)}\")\nprint(f\"   Sarcastic: {len(final_df[final_df['sarcasm']==1])}\")\nprint(f\"   Non-sarcastic: {len(final_df[final_df['sarcasm']==0])}\")\nprint(f\"   Missing processed_text: {final_df['processed_text'].isna().sum()}\")\n\n# Show sample of processed data\nprint(f\"\\nüìù Sample of processed data:\")\nfor i in range(min(3, len(final_df))):\n    row = final_df.iloc[i]\n    print(f\"\\nSample {i+1}:\")\n    print(f\"  Original:  {row['text'][:100]}...\")\n    print(f\"  Processed: {row['processed_text'][:100]}...\")\n    print(f\"  Label: {row['sarcasm']}\")\n\nprint(f\"\\nüéØ MISSION ACCOMPLISHED - PERSON 1 TASK COMPLETE!\")\nprint(f\"üì§ Download the file and share with team for training phase\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T15:39:21.704065Z","iopub.execute_input":"2025-06-19T15:39:21.704330Z","iopub.status.idle":"2025-06-19T15:39:21.720404Z","shell.execute_reply.started":"2025-06-19T15:39:21.704310Z","shell.execute_reply":"2025-06-19T15:39:21.719532Z"}},"outputs":[{"name":"stdout","text":"=== FINAL VERIFICATION ===\nüìä Dataset Summary:\n   Total samples: 15\n   Sarcastic: 15\n   Non-sarcastic: 0\n   Missing processed_text: 0\n\nüìù Sample of processed data:\n\nSample 1:\n  Original:  <user> thanks for showing up for our appointment today . ...\n  Processed: Thanks for showing up for our appointment today....\n  Label: 1\n\nSample 2:\n  Original:  haha .  # lol...\n  Processed: Laughing out loud...\n  Label: 1\n\nSample 3:\n  Original:  i love waiting <num> min for a cab - such shortage .......  <user> please allow uber . this is insan...\n  Processed: I hate waiting for a cab - such shortage. Please allow Uber. This is insane. üöñüöñüöñüöñ...\n  Label: 1\n\nüéØ MISSION ACCOMPLISHED - PERSON 1 TASK COMPLETE!\nüì§ Download the file and share with team for training phase\n","output_type":"stream"}],"execution_count":7}]}