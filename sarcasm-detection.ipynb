{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9560502,"sourceType":"datasetVersion","datasetId":5826041}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Instalasi library yang diperlukan dari Hugging Face\n!pip install -q transformers ftfy regex\n!pip install --upgrade -q transformers\n!pip install -q accelerate\n\n# Impor library standar dan dari Hugging Face\nimport os\nimport json\nimport torch\nimport pandas as pd\nimport re\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom transformers import CLIPProcessor, CLIPModel\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Cek ketersediaan GPU\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\n\n# Path ke direktori dataset di Kaggle\ndata_dir = \"/kaggle/input/data-of-multimodal-sarcasm-detection\"\n\n# Fungsi untuk memuat data dari file .txt\ndef load_data_from_txt(filepath):\n    records = []\n    with open(filepath, 'r', encoding='utf-8') as f:\n        for line in f:\n            try:\n                # Menggunakan ast.literal_eval untuk mengubah string menjadi list\n                # Contoh: \"['id', 'teks', 1, 1]\" -> ['id', 'teks', 1, 1]\n                data_list = ast.literal_eval(line.strip())\n\n                # Ekstrak data berdasarkan posisi di dalam list\n                tweet_id = data_list[0]\n                text = data_list[1]\n                label = int(data_list[2]) # Label sarkasme ada di posisi ke-3\n\n                records.append({'id': tweet_id, 'text': text, 'sarcasm': label})\n            except (ValueError, SyntaxError):\n                # Lewati baris yang formatnya rusak atau tidak bisa di-parse\n                # print(f\"Skipping malformed line: {line}\")\n                continue\n    return pd.DataFrame(records)\n\n# Memuat data dari masing-masing file\ntrain_df = load_data_from_txt(os.path.join(data_dir, 'text', 'train.txt'))\nval_df = load_data_from_txt(os.path.join(data_dir, 'text', 'valid2.txt'))\ntest_df = load_data_from_txt(os.path.join(data_dir, 'text', 'test2.txt'))\n\n# Membuat path lengkap untuk setiap gambar\nimage_folder = os.path.join(data_dir, 'dataset_image')\ntrain_df['image_path'] = train_df['id'].apply(lambda x: os.path.join(image_folder, f\"{x}.jpg\"))\nval_df['image_path'] = val_df['id'].apply(lambda x: os.path.join(image_folder, f\"{x}.jpg\"))\ntest_df['image_path'] = test_df['id'].apply(lambda x: os.path.join(image_folder, f\"{x}.jpg\"))\n\n# Memastikan hanya baris dengan gambar yang ada yang diproses\ntrain_df = train_df[train_df['image_path'].apply(os.path.exists)].dropna()\nval_df = val_df[val_df['image_path'].apply(os.path.exists)].dropna()\ntest_df = test_df[test_df['image_path'].apply(os.path.exists)].dropna()\n\n# Mengubah tipe data kolom 'sarcasm' menjadi integer\ntrain_df['sarcasm'] = train_df['sarcasm'].astype(int)\nval_df['sarcasm'] = val_df['sarcasm'].astype(int)\ntest_df['sarcasm'] = test_df['sarcasm'].astype(int)\n\n# --- PERUBAHAN DI SINI ---\nsarcastic_samples = train_df[train_df['sarcasm'] == 1].sample(n=16, random_state=42)\nnon_sarcastic_samples = train_df[train_df['sarcasm'] == 0].sample(n=16, random_state=42)\ntrain_df_16shot = pd.concat([sarcastic_samples, non_sarcastic_samples])\n\nprint(f\"Ukuran data latih (16-shot): {len(train_df_512shot)}\")\nprint(f\"Ukuran data validasi: {len(val_df)}\")\nprint(f\"Ukuran data uji: {len(test_df)}\")\n\n# Tampilkan beberapa contoh data\ntrain_df_16shot.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SarcasmDataset(Dataset):\n    def __init__(self, dataframe, processor):\n        self.dataframe = dataframe\n        self.processor = processor\n        self.texts = dataframe['text'].tolist()\n        self.image_paths = dataframe['image_path'].tolist()\n        self.labels = dataframe['sarcasm'].tolist()\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        image = Image.open(self.image_paths[idx]).convert(\"RGB\")\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n\n        # --- BAGIAN YANG DIPERBAIKI ---\\n        # Proses input dengan padding ke panjang maksimal dan truncation\n        # Ini memastikan semua output tensor teks memiliki ukuran yang sama\n        inputs = self.processor(\n            text=[text],\n            images=image,\n            return_tensors=\"pt\",\n            padding=\"max_length\",  # Ubah dari True menjadi \"max_length\"\n            max_length=77,         # Panjang standar untuk model CLIP\n            truncation=True        # Pastikan truncation aktif\n        )\n        # -----------------------------\\n\n        return {\n            'input_ids': inputs['input_ids'].squeeze(0),\n            'attention_mask': inputs['attention_mask'].squeeze(0),\n            'pixel_values': inputs['pixel_values'].squeeze(0),\n            'labels': label\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CueLearningSarcasmModel(nn.Module):\n    def __init__(self, clip_model_name=\"openai/clip-vit-large-patch14\"):\n        super().__init__()\n        self.clip = CLIPModel.from_pretrained(clip_model_name)\n        self.processor = CLIPProcessor.from_pretrained(clip_model_name)\n\n        for param in self.clip.parameters():\n            param.requires_grad = False\n\n        text_prompt_length = 12\n        image_prompt_length = 20\n        sarcasm_prompt_length = 8\n        d_model = self.clip.text_embed_dim\n\n        self.text_prompts = nn.Parameter(torch.randn(text_prompt_length, d_model))\n        self.image_prompts = nn.Parameter(torch.randn(image_prompt_length, self.clip.vision_embed_dim))\n\n        sarcasm_texts = [\"a sarcastic tweet\", \"this is sarcasm\"]\n        non_sarcasm_texts = [\"a normal tweet\", \"this is not sarcasm\"]\n        sarcasm_tokens = self.processor(text=sarcasm_texts, return_tensors=\"pt\", padding=True, truncation=True)\n        non_sarcasm_tokens = self.processor(text=non_sarcasm_texts, return_tensors=\"pt\", padding=True, truncation=True)\n\n        with torch.no_grad():\n            sarcasm_word_embeds = self.clip.text_model.embeddings.token_embedding(sarcasm_tokens.input_ids).mean(dim=0)\n            non_sarcasm_word_embeds = self.clip.text_model.embeddings.token_embedding(non_sarcasm_tokens.input_ids).mean(dim=0)\n\n        self.sarcasm_prompt_embeds = nn.Parameter(torch.cat([\n            torch.randn(sarcasm_prompt_length, d_model),\n            sarcasm_word_embeds\n        ], dim=0))\n        self.non_sarcasm_prompt_embeds = nn.Parameter(torch.cat([\n            torch.randn(sarcasm_prompt_length, d_model),\n            non_sarcasm_word_embeds\n        ], dim=0))\n\n    def _prepare_4d_attention_mask(self, mask_2d, dtype, device):\n        \"\"\"Mempersiapkan 2D padding mask menjadi 4D additive mask.\"\"\"\n        # Ubah 2D mask [B, S] menjadi 4D [B, 1, 1, S]\n        mask_4d = mask_2d.to(dtype).unsqueeze(1).unsqueeze(1)\n        # Invert: 1 -> 0, 0 -> 1\n        inverted_mask = 1.0 - mask_4d\n        # Ubah menjadi additive mask: 0 -> 0, 1 -> -inf\n        return inverted_mask * torch.finfo(dtype).min\n\n    def _prepare_4d_causal_attention_mask(self, shape, dtype, device):\n        \"\"\"Mempersiapkan 4D causal mask.\"\"\"\n        bsz, seq_len = shape[0], shape[1]\n        # Buat matriks segitiga atas berisi -inf\n        causal_mask = torch.empty((bsz, seq_len, seq_len), dtype=dtype, device=device)\n        causal_mask.fill_(torch.finfo(dtype).min)\n        causal_mask.triu_(1)\n        # Tambah dimensi untuk multi-head attention\n        return causal_mask.unsqueeze(1)\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        # 1. Dapatkan embedding awal\n        inputs_embeds = self.clip.text_model.embeddings.token_embedding(input_ids)\n        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n        image_embeds = vision_outputs[1]\n\n        # 2. Tambahkan prompt ke text embedding\n        prompted_text_embeds = torch.cat([\n            self.text_prompts.unsqueeze(0).expand(inputs_embeds.shape[0], -1, -1),\n            inputs_embeds\n        ], dim=1)\n\n        # 3. Buat padding mask 2D yang diperluas\n        prompt_attention_mask = torch.ones(\n            prompted_text_embeds.shape[0], self.text_prompts.shape[0],\n            dtype=attention_mask.dtype, device=input_ids.device\n        )\n        extended_attention_mask_2d = torch.cat([prompt_attention_mask, attention_mask], dim=1)\n\n        # === FIX FINAL: Buat dan gabungkan mask secara manual ===\n        # Buat 4D padding mask dari 2D mask\n        padding_mask_4d = self._prepare_4d_attention_mask(\n            extended_attention_mask_2d, prompted_text_embeds.dtype, input_ids.device\n        )\n        # Buat 4D causal mask\n        causal_mask_4d = self._prepare_4d_causal_attention_mask(\n            prompted_text_embeds.shape, prompted_text_embeds.dtype, input_ids.device\n        )\n        # Gabungkan keduanya menjadi satu mask final\n        final_attention_mask = padding_mask_4d + causal_mask_4d\n        # === END FIX FINAL ===\n\n        # 4. Proses melalui text encoder dengan satu mask final\n        text_encoder_outputs = self.clip.text_model.encoder(\n            inputs_embeds=prompted_text_embeds,\n            attention_mask=final_attention_mask,  # Hanya gunakan mask ini\n        )\n        last_hidden_state = text_encoder_outputs[0]\n        normed_hidden_state = self.clip.text_model.final_layer_norm(last_hidden_state)\n\n        eos_token_pos = input_ids.argmax(dim=-1)\n        shifted_eos_pos = eos_token_pos + self.text_prompts.shape[0]\n        batch_indices = torch.arange(normed_hidden_state.shape[0], device=input_ids.device)\n        text_features = normed_hidden_state[batch_indices, shifted_eos_pos]\n\n        # 5. Proses image features\n        image_features = self.clip.visual_projection(image_embeds)\n        text_features_proj = self.clip.text_projection(text_features)\n\n        # 6. Fusion multi-modal\n        multi_modal_features = (text_features_proj + image_features) / 2.0\n        multi_modal_features = F.normalize(multi_modal_features, p=2, dim=-1)\n\n        # 7. Dapatkan fitur untuk prompt sarkasme/non-sarkasme\n        def get_prompt_features(prompt_embeds):\n            prompt_embeds_b1 = prompt_embeds.unsqueeze(0)\n            # Karena prompt ini tidak di-padding, kita hanya butuh causal mask\n            causal_mask = self._prepare_4d_causal_attention_mask(\n                prompt_embeds_b1.shape, prompt_embeds_b1.dtype, prompt_embeds_b1.device\n            )\n            encoder_out = self.clip.text_model.encoder(\n                inputs_embeds=prompt_embeds_b1,\n                attention_mask=causal_mask\n            )\n            normed_out = self.clip.text_model.final_layer_norm(encoder_out[0])\n            features = normed_out[:, -1, :]\n            return self.clip.text_projection(features)\n\n        sarcasm_prompt_features = get_prompt_features(self.sarcasm_prompt_embeds)\n        non_sarcasm_prompt_features = get_prompt_features(self.non_sarcasm_prompt_embeds)\n\n        sarcasm_prompt_features = F.normalize(sarcasm_prompt_features, p=2, dim=-1)\n        non_sarcasm_prompt_features = F.normalize(non_sarcasm_prompt_features, p=2, dim=-1)\n\n        # 8. Hitung Probabilitas Sarkasme\n        sim_sarcasm = F.cosine_similarity(multi_modal_features, sarcasm_prompt_features.squeeze(0))\n        sim_non_sarcasm = F.cosine_similarity(multi_modal_features, non_sarcasm_prompt_features.squeeze(0))\n\n        logits = torch.stack([sim_non_sarcasm, sim_sarcasm], dim=1) * self.clip.logit_scale.exp()\n        return logits","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Inisialisasi model dan pindahkan ke GPU\nmodel = CueLearningSarcasmModel().to(device)\nprocessor = model.processor\n\n# --- PERUBAHAN DI SINI ---\n# Membuat instance Dataset dan DataLoader menggunakan data 512-shot\ntrain_dataset = SarcasmDataset(train_df_512shot, processor)\nval_dataset = SarcasmDataset(val_df, processor)\n\n# Ukuran batch disesuaikan untuk dataset yang lebih besar\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n\n# Optimizer (hanya melatih parameter prompt yang kita buat)\nlearnable_params = [p for p in model.parameters() if p.requires_grad]\noptimizer = AdamW(learnable_params, lr=2e-3) # Learning rate dari paper\ncriterion = nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\n\n# Re-initialize the model to ensure the fix is applied\nmodel = CueLearningSarcasmModel().to(device)\nprocessor = model.processor\n\n# --- PERUBAHAN DI SINI ---\n# Re-create Datasets and DataLoaders menggunakan data 512-shot\ntrain_dataset = SarcasmDataset(train_df_16shot, processor)\nval_dataset = SarcasmDataset(val_df, processor)\n\n# Ukuran batch disesuaikan\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32)\n\n# Optimizer (only trains the learnable prompt parameters)\nlearnable_params = [p for p in model.parameters() if p.requires_grad]\noptimizer = AdamW(learnable_params, lr=2e-3)\ncriterion = nn.CrossEntropyLoss()\n\n# --- Training and Evaluation Loop ---\nnum_epochs = 1 # Anda mungkin ingin menyesuaikan jumlah epoch\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for batch in progress_bar:\n        # Move batch data to the device\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Forward pass\n        outputs = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        progress_bar.set_postfix({'loss': loss.item()})\n\n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1} | Average Training Loss: {avg_train_loss:.4f}\")\n\n    # --- Evaluation ---\n    model.eval()\n    all_preds = []\n    all_labels = []\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask, pixel_values)\n            preds = torch.argmax(outputs, dim=1)\n\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds)\n    print(f\"Validation Accuracy: {acc:.4f} | Validation F1-Score: {f1:.4f}\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}