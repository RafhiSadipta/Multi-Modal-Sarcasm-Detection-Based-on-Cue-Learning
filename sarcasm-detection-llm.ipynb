{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9560502,"sourceType":"datasetVersion","datasetId":5826041},{"sourceId":12196742,"sourceType":"datasetVersion","datasetId":7682864}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- 0. KAGGLE ENVIRONMENT SETUP ---\nimport os\nimport sys\n\nprint(\"=== KAGGLE ENVIRONMENT SETUP ===\")\nprint(f\"Python version: {sys.version.split()[0]}\")\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Validate Kaggle environment\nassert '/kaggle/' in os.getcwd(), \"This notebook is designed specifically for Kaggle environment!\"\nprint(\"‚úì Confirmed running in Kaggle environment\")\n\n# Check available datasets\ninput_dir = \"/kaggle/input\"\nif os.path.exists(input_dir):\n    datasets = os.listdir(input_dir)\n    print(f\"Available datasets: {datasets}\")\n    \n    # Look for sarcasm detection dataset\n    sarcasm_datasets = [d for d in datasets if 'sarcasm' in d.lower() or 'multimodal' in d.lower()]\n    if sarcasm_datasets:\n        print(f\"‚úì Found sarcasm datasets: {sarcasm_datasets}\")\n    else:\n        print(\"‚ö† Warning: No sarcasm detection dataset found. Please add the dataset to this notebook.\")\nelse:\n    print(\"‚ö† Warning: No input datasets found\")\n\n# Setup working directory\nos.makedirs(\"/kaggle/working/processed_data\", exist_ok=True)\nos.makedirs(\"/kaggle/working/models\", exist_ok=True)\n\nprint(\"‚úì Kaggle environment ready\")\nprint(\"=\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. INSTALASI DAN IMPORT LIBRARY ---\nprint(\"Menginstal library yang diperlukan...\")\n\n# Install dengan suppressed output untuk menghindari spam\nimport subprocess\nimport sys\n\ndef install_package(package):\n    \"\"\"Install package with error handling\"\"\"\n    try:\n        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"Warning: Failed to install {package}: {e}\")\n        return False\n\n# List packages yang diperlukan\nrequired_packages = [\n    \"transformers\", \n    \"ftfy\", \n    \"regex\", \n    \"accelerate\", \n    \"openai\"\n]\n\n# Install packages\nfor package in required_packages:\n    print(f\"Installing {package}...\", end=\" \")\n    if install_package(package):\n        print(\"‚úì\")\n    else:\n        print(\"‚úó\")\n\nprint(\"\\nImporting libraries...\")\n\n# Import libraries with error handling\ntry:\n    import os\n    import json\n    import torch\n    import pandas as pd\n    import re\n    import time\n    from torch.utils.data import Dataset, DataLoader\n    from PIL import Image\n    from transformers import CLIPProcessor, CLIPModel\n    from torch.optim import AdamW\n    from sklearn.model_selection import train_test_split\n    from tqdm.notebook import tqdm\n    import torch.nn as nn\n    import torch.nn.functional as F\n    from sklearn.metrics import accuracy_score, f1_score\n    from openai import OpenAI\n    \n    print(\"‚úì All libraries imported successfully\")\n    \nexcept ImportError as e:\n    print(f\"Error importing libraries: {e}\")\n    print(\"Please make sure all required packages are installed\")\n    raise\n\n# Set random seeds untuk reproducibility\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. KONFIGURASI UMUM DAN GPU ---\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. DATA LOADING FROM SEPARATE PREPROCESSED CSV FILES ---\nprint(\"Loading separate preprocessed CSV files from team preprocessing...\")\n\n# Configuration for different shot variations\nSHOT_VARIATION = \"16shot\"  # Options: \"16shot\", \"64shot\", \"128shot\", \"512shot\", \"1024shot\", \"all\"\n\nprint(f\"üéØ Selected variation: {SHOT_VARIATION}\")\n\n# Shot configuration mapping\nshot_configs = {\n    \"16shot\": 16,\n    \"128shot\": 128,\n    \"512shot\": 512,\n    \"1024shot\": 1024,\n    \"all\": None  # Use all available data\n}\n\nsamples_per_class = shot_configs.get(SHOT_VARIATION)\nif samples_per_class:\n    print(f\"üìä Using {samples_per_class} samples per class (total: {samples_per_class * 2})\")\nelse:\n    print(f\"üìä Using all available training data\")\n\n# Expected preprocessed files from team (SEPARATE FILES)\npreprocessed_files = {\n    'train': [\n        '/kaggle/input/preprocessed-sarcasm-text/train_processed_complete.csv',  # Person 1 output\n    ],\n    'validation': [\n        '/kaggle/input/preprocessed-sarcasm-text/validation_processed_complete.csv',  # Person 2 output\n    ],\n    'test': [\n        '/kaggle/input/preprocessed-sarcasm-text/test_processed_complete.csv',  # Person 3 output\n    ]\n}\n\ndef load_preprocessed_dataset(dataset_name, file_paths):\n    \"\"\"Load a single preprocessed dataset with fallback paths\"\"\"\n    for file_path in file_paths:\n        if os.path.exists(file_path):\n            print(f\"‚úì Loading {dataset_name} data from: {file_path}\")\n            try:\n                df = pd.read_csv(file_path)\n                \n                # Validate required columns\n                required_cols = ['id', 'text', 'processed_text', 'sarcasm', 'image_path']\n                missing_cols = [col for col in required_cols if col not in df.columns]\n                if missing_cols:\n                    print(f\"‚ö†Ô∏è  {dataset_name} missing columns: {missing_cols}, trying next path...\")\n                    continue\n                \n                print(f\"   üìã Loaded {len(df)} samples\")\n                print(f\"   üìä Sarcastic: {len(df[df['sarcasm']==1])}, Non-sarcastic: {len(df[df['sarcasm']==0])}\")\n                \n                # Check processed text quality\n                missing_processed = df['processed_text'].isna().sum()\n                if missing_processed > 0:\n                    print(f\"   ‚ö†Ô∏è  Missing processed_text: {missing_processed}\")\n                else:\n                    print(f\"   ‚úÖ All samples have processed text\")\n                \n                return df\n                \n            except Exception as e:\n                print(f\"   ‚ùå Error loading {file_path}: {e}\")\n                continue\n    \n    # If no file found\n    raise FileNotFoundError(f\"‚ùå {dataset_name} data not found in any of the expected paths: {file_paths}\")\n\n# Load all datasets separately\nprint(\"\\n=== LOADING SEPARATE PREPROCESSED DATASETS ===\")\n\ntrain_df = load_preprocessed_dataset(\"Training\", preprocessed_files['train'])\nval_df = load_preprocessed_dataset(\"Validation\", preprocessed_files['validation'])  \ntest_df = load_preprocessed_dataset(\"Test\", preprocessed_files['test'])\n\nprint(f\"\\n‚úÖ All datasets loaded successfully!\")\n\n# Apply shot variation to training data\nprint(f\"\\nüîß Applying {SHOT_VARIATION} configuration to training data...\")\n\nif samples_per_class and SHOT_VARIATION != \"all\":\n    # Separate by class\n    sarcastic_train = train_df[train_df['sarcasm'] == 1]\n    non_sarcastic_train = train_df[train_df['sarcasm'] == 0]\n    \n    print(f\"  Available - Sarcastic: {len(sarcastic_train)}, Non-sarcastic: {len(non_sarcastic_train)}\")\n    \n    # Sample from each class\n    if len(sarcastic_train) < samples_per_class:\n        print(f\"‚ö†Ô∏è  Warning: Only {len(sarcastic_train)} sarcastic samples available, using all\")\n        selected_sarcastic = sarcastic_train\n    else:\n        selected_sarcastic = sarcastic_train.sample(n=samples_per_class, random_state=42)\n    \n    if len(non_sarcastic_train) < samples_per_class:\n        print(f\"‚ö†Ô∏è  Warning: Only {len(non_sarcastic_train)} non-sarcastic samples available, using all\")\n        selected_non_sarcastic = non_sarcastic_train\n    else:\n        selected_non_sarcastic = non_sarcastic_train.sample(n=samples_per_class, random_state=42)\n    \n    # Combine selected samples\n    train_df = pd.concat([selected_sarcastic, selected_non_sarcastic]).sample(frac=1, random_state=42).reset_index(drop=True)\n    \n    print(f\"  Selected - Sarcastic: {len(selected_sarcastic)}, Non-sarcastic: {len(selected_non_sarcastic)}\")\n    print(f\"  Final training set: {len(train_df)} samples\")\nelse:\n    print(f\"  Using all {len(train_df)} training samples\")\n\n# Final summary\nprint(f\"\\n=== FINAL DATASET SUMMARY ===\")\nprint(f\"Configuration: {SHOT_VARIATION}\")\nprint(f\"Training samples: {len(train_df)}\")\nprint(f\"  - Sarcastic: {len(train_df[train_df['sarcasm']==1])}\")\nprint(f\"  - Non-sarcastic: {len(train_df[train_df['sarcasm']==0])}\")\nprint(f\"Validation samples: {len(val_df)}\")\nprint(f\"  - Sarcastic: {len(val_df[val_df['sarcasm']==1])}\")\nprint(f\"  - Non-sarcastic: {len(val_df[val_df['sarcasm']==0])}\")\nprint(f\"Test samples: {len(test_df)}\")\nprint(f\"  - Sarcastic: {len(test_df[test_df['sarcasm']==1])}\")\nprint(f\"  - Non-sarcastic: {len(test_df[test_df['sarcasm']==0])}\")\n\n# Verify processed text quality across all datasets\nmissing_processed_train = train_df['processed_text'].isna().sum()\nmissing_processed_val = val_df['processed_text'].isna().sum() \nmissing_processed_test = test_df['processed_text'].isna().sum()\n\nprint(f\"\\nüîç Data Quality Check:\")\nprint(f\"Missing processed_text - Train: {missing_processed_train}, Val: {missing_processed_val}, Test: {missing_processed_test}\")\n\ntotal_missing = missing_processed_train + missing_processed_val + missing_processed_test\ntotal_samples = len(train_df) + len(val_df) + len(test_df)\n\nif total_missing == 0:\n    print(\"‚úÖ All datasets have complete processed text - LLM preprocessing successful!\")\n    print(\"ü§ù Team preprocessing workflow completed successfully!\")\nelse:\n    print(f\"‚ö†Ô∏è  {total_missing}/{total_samples} missing processed text detected\")\n    print(\"üìã This is normal for fallback cases where LLM processing failed\")\n\nprint(f\"\\nüìà Ready for {SHOT_VARIATION} training with preprocessed data!\")\nprint(\"=\" * 60)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5.1 DATA VALIDATION AND CLEANING ---\nprint(\"=== DATA VALIDATION AND CLEANING ===\")\n\ndef validate_and_clean_text_data(df, dataset_name):\n    \"\"\"Validate and clean text data to prevent tokenizer errors\"\"\"\n    print(f\"\\nüîç Validating {dataset_name} dataset...\")\n    \n    original_count = len(df)\n    print(f\"   Original samples: {original_count}\")\n    \n    # Check text column\n    if 'text' in df.columns:\n        null_text = df['text'].isna().sum()\n        empty_text = (df['text'].str.strip() == '').sum()\n        print(f\"   Null text: {null_text}\")\n        print(f\"   Empty text: {empty_text}\")\n    \n    # Check processed_text column\n    if 'processed_text' in df.columns:\n        null_processed = df['processed_text'].isna().sum()\n        empty_processed = (df['processed_text'].str.strip() == '').sum()\n        print(f\"   Null processed_text: {null_processed}\")\n        print(f\"   Empty processed_text: {empty_processed}\")\n        \n        # Fill missing processed_text with original text\n        df['processed_text'] = df['processed_text'].fillna(df['text'])\n        \n        # Check after filling\n        still_null = df['processed_text'].isna().sum()\n        if still_null > 0:\n            print(f\"   ‚ö†Ô∏è  Still {still_null} null values after filling, using fallback\")\n            df['processed_text'] = df['processed_text'].fillna(\"No text available\")\n    else:\n        # Create processed_text from text if not exists\n        df['processed_text'] = df['text'].fillna(\"No text available\")\n        print(f\"   Created processed_text from text column\")\n    \n    # Ensure all text values are valid strings\n    df['processed_text'] = df['processed_text'].astype(str)\n    df['text'] = df['text'].astype(str)\n    \n    # Replace empty strings with fallback\n    df.loc[df['processed_text'].str.strip() == '', 'processed_text'] = \"No text available\"\n    df.loc[df['text'].str.strip() == '', 'text'] = \"No text available\"\n    \n    final_count = len(df)\n    print(f\"   Final samples: {final_count}\")\n    print(f\"   ‚úÖ All text values validated and cleaned\")\n    \n    return df\n\n# Validate and clean all datasets\nprint(\"üßπ Cleaning datasets to prevent tokenizer errors...\")\n\ntrain_df = validate_and_clean_text_data(train_df, \"Training\")\nval_df = validate_and_clean_text_data(val_df, \"Validation\") \ntest_df = validate_and_clean_text_data(test_df, \"Test\")\n\nprint(\"\\nüîç Final validation check:\")\nfor name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n    null_text = df['processed_text'].isna().sum()\n    empty_text = (df['processed_text'].str.strip() == '').sum()\n    print(f\"   {name}: {null_text} null, {empty_text} empty processed_text values\")\n\nprint(\"‚úÖ Data validation and cleaning completed!\")\nprint(\"=\" * 60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 6. PYTORCH DATASET CLASS (KAGGLE OPTIMIZED) ---\nclass SarcasmDataset(Dataset):\n    def __init__(self, dataframe, processor):\n        self.dataframe = dataframe.copy()\n        self.processor = processor\n        \n        # Use processed_text if available, otherwise use original text\n        if 'processed_text' in dataframe.columns:\n            # Handle missing processed_text values properly\n            processed_texts = dataframe['processed_text'].fillna(dataframe['text'])\n            original_texts = dataframe['text'].fillna(\"No text available\")\n            \n            # Combine processed and original, ensure no null values\n            self.texts = []\n            for proc_text, orig_text in zip(processed_texts, original_texts):\n                if pd.isna(proc_text) or not isinstance(proc_text, str) or not proc_text.strip():\n                    if pd.isna(orig_text) or not isinstance(orig_text, str) or not orig_text.strip():\n                        self.texts.append(\"No text available\")\n                    else:\n                        self.texts.append(str(orig_text).strip())\n                else:\n                    self.texts.append(str(proc_text).strip())\n            \n            print(f\"‚úì Using processed_text for {len(self.texts)} samples\")\n        else:\n            # Fallback to original text with null handling\n            original_texts = dataframe['text'].fillna(\"No text available\")\n            self.texts = [str(text).strip() if pd.notna(text) and str(text).strip() else \"No text available\" \n                         for text in original_texts]\n            print(f\"‚ö† Using original text for {len(self.texts)} samples\")\n            \n        self.image_paths = dataframe['image_path'].tolist()\n        self.labels = dataframe['sarcasm'].tolist()\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        image_path = self.image_paths[idx]\n        label = torch.tensor(self.labels[idx], dtype=torch.long)\n        \n        # Handle null/NaN text values\n        if pd.isna(text) or text is None or not isinstance(text, str):\n            text = \"No text available\"  # Fallback text\n        \n        # Ensure text is a proper string\n        text = str(text).strip()\n        if not text:  # Empty string\n            text = \"No text available\"\n        \n        # Load image with fallback\n        try:\n            image = Image.open(image_path).convert(\"RGB\")\n        except:\n            # Create white dummy image if file missing\n            image = Image.new('RGB', (224, 224), color='white')\n        \n        # Process with CLIP\n        inputs = self.processor(\n            text=[text], \n            images=image, \n            return_tensors=\"pt\", \n            padding=\"max_length\", \n            max_length=77, \n            truncation=True\n        )\n        \n        return {\n            'input_ids': inputs['input_ids'].squeeze(0), \n            'attention_mask': inputs['attention_mask'].squeeze(0), \n            'pixel_values': inputs['pixel_values'].squeeze(0), \n            'labels': label\n        }","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 7. KELAS MODEL (VERSI FINAL YANG SUDAH DIPERBAIKI) ---\nclass CueLearningSarcasmModel(nn.Module):\n    def __init__(self, clip_model_name=\"openai/clip-vit-large-patch14\"):\n        super().__init__()\n        self.clip = CLIPModel.from_pretrained(clip_model_name)\n        self.processor = CLIPProcessor.from_pretrained(clip_model_name)\n        for param in self.clip.parameters():\n            param.requires_grad = False\n        text_prompt_length, image_prompt_length, sarcasm_prompt_length = 12, 20, 8\n        d_model = self.clip.text_embed_dim\n        self.text_prompts = nn.Parameter(torch.randn(text_prompt_length, d_model))\n        self.image_prompts = nn.Parameter(torch.randn(image_prompt_length, self.clip.vision_embed_dim))\n        sarcasm_texts, non_sarcasm_texts = [\"a sarcastic tweet\", \"this is sarcasm\"], [\"a normal tweet\", \"this is not sarcasm\"]\n        sarcasm_tokens = self.processor(text=sarcasm_texts, return_tensors=\"pt\", padding=True, truncation=True)\n        non_sarcasm_tokens = self.processor(text=non_sarcasm_texts, return_tensors=\"pt\", padding=True, truncation=True)\n        with torch.no_grad():\n            sarcasm_word_embeds = self.clip.text_model.embeddings.token_embedding(sarcasm_tokens.input_ids).mean(dim=0)\n            non_sarcasm_word_embeds = self.clip.text_model.embeddings.token_embedding(non_sarcasm_tokens.input_ids).mean(dim=0)\n        self.sarcasm_prompt_embeds = nn.Parameter(torch.cat([torch.randn(sarcasm_prompt_length, d_model), sarcasm_word_embeds], dim=0))\n        self.non_sarcasm_prompt_embeds = nn.Parameter(torch.cat([torch.randn(sarcasm_prompt_length, d_model), non_sarcasm_word_embeds], dim=0))\n\n    def _prepare_4d_attention_mask(self, mask_2d, dtype, device):\n        mask_4d = mask_2d.to(dtype).unsqueeze(1).unsqueeze(1)\n        inverted_mask = 1.0 - mask_4d\n        return inverted_mask * torch.finfo(dtype).min\n\n    def _prepare_4d_causal_attention_mask(self, shape, dtype, device):\n        bsz, seq_len = shape[0], shape[1]\n        causal_mask = torch.empty((bsz, seq_len, seq_len), dtype=dtype, device=device)\n        causal_mask.fill_(torch.finfo(dtype).min)\n        causal_mask.triu_(1)\n        return causal_mask.unsqueeze(1)\n\n    def forward(self, input_ids, attention_mask, pixel_values):\n        inputs_embeds = self.clip.text_model.embeddings.token_embedding(input_ids)\n        vision_outputs = self.clip.vision_model(pixel_values=pixel_values)\n        image_embeds = vision_outputs[1]\n        prompted_text_embeds = torch.cat([self.text_prompts.unsqueeze(0).expand(inputs_embeds.shape[0], -1, -1), inputs_embeds], dim=1)\n        prompt_attention_mask = torch.ones(prompted_text_embeds.shape[0], self.text_prompts.shape[0], dtype=attention_mask.dtype, device=input_ids.device)\n        extended_attention_mask_2d = torch.cat([prompt_attention_mask, attention_mask], dim=1)\n        \n        padding_mask_4d = self._prepare_4d_attention_mask(extended_attention_mask_2d, prompted_text_embeds.dtype, input_ids.device)\n        causal_mask_4d = self._prepare_4d_causal_attention_mask(prompted_text_embeds.shape, prompted_text_embeds.dtype, input_ids.device)\n        final_attention_mask = padding_mask_4d + causal_mask_4d\n        \n        text_encoder_outputs = self.clip.text_model.encoder(inputs_embeds=prompted_text_embeds, attention_mask=final_attention_mask)\n        last_hidden_state = self.clip.text_model.final_layer_norm(text_encoder_outputs[0])\n        \n        shifted_eos_pos = input_ids.argmax(dim=-1) + self.text_prompts.shape[0]\n        text_features = last_hidden_state[torch.arange(last_hidden_state.shape[0], device=input_ids.device), shifted_eos_pos]\n        \n        image_features = self.clip.visual_projection(image_embeds)\n        text_features_proj = self.clip.text_projection(text_features)\n        multi_modal_features = F.normalize((text_features_proj + image_features) / 2.0, p=2, dim=-1)\n\n        def get_prompt_features(prompt_embeds):\n            prompt_embeds_b1 = prompt_embeds.unsqueeze(0)\n            causal_mask = self._prepare_4d_causal_attention_mask(prompt_embeds_b1.shape, prompt_embeds_b1.dtype, prompt_embeds_b1.device)\n            encoder_out = self.clip.text_model.encoder(inputs_embeds=prompt_embeds_b1, attention_mask=causal_mask)\n            features = self.clip.text_model.final_layer_norm(encoder_out[0])[:, -1, :]\n            return self.clip.text_projection(features)\n\n        sarcasm_prompt_features = F.normalize(get_prompt_features(self.sarcasm_prompt_embeds), p=2, dim=-1)\n        non_sarcasm_prompt_features = F.normalize(get_prompt_features(self.non_sarcasm_prompt_embeds), p=2, dim=-1)\n        \n        sim_sarcasm = F.cosine_similarity(multi_modal_features, sarcasm_prompt_features.squeeze(0))\n        sim_non_sarcasm = F.cosine_similarity(multi_modal_features, non_sarcasm_prompt_features.squeeze(0))\n        logits = torch.stack([sim_non_sarcasm, sim_sarcasm], dim=1) * self.clip.logit_scale.exp()\n        return logits","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 8. TRAINING & EVALUATION (KAGGLE OPTIMIZED) ---\nprint(\"=== TRAINING SETUP ===\")\n\n# Initialize model\nmodel = CueLearningSarcasmModel().to(device)\nprocessor = model.processor\n\n# Create datasets\ntrain_dataset = SarcasmDataset(train_df, processor)\nval_dataset = SarcasmDataset(val_df, processor)\n\n# Optimized data loaders for 16-shot\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n\nprint(f\"Training batches: {len(train_loader)}\")\nprint(f\"Validation batches: {len(val_loader)}\")\n\n# Setup optimizer\nlearnable_params = [p for p in model.parameters() if p.requires_grad]\nprint(f\"Learnable parameters: {sum(p.numel() for p in learnable_params):,}\")\n\noptimizer = AdamW(learnable_params, lr=2e-3, weight_decay=1e-4)\ncriterion = nn.CrossEntropyLoss()\n\n# Training parameters optimized for 16-shot\nnum_epochs = 1  # Reduced for faster Kaggle execution\nbest_val_acc = 0.0\nmodel_save_path = \"/kaggle/working/models/best_model.pth\"\n\nprint(f\"\\n=== STARTING 1024-SHOT TRAINING ({num_epochs} epochs) ===\")\n\n# Training loop\nfor epoch in range(num_epochs):\n    # Training phase\n    model.train()\n    total_loss = 0\n    train_correct = 0\n    train_total = 0\n    \n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device) \n        pixel_values = batch['pixel_values'].to(device)\n        labels = batch['labels'].to(device)\n        \n        # Forward pass\n        outputs = model(input_ids, attention_mask, pixel_values)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(learnable_params, max_norm=1.0)\n        optimizer.step()\n        \n        # Statistics\n        total_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        train_total += labels.size(0)\n        train_correct += (predicted == labels).sum().item()\n\n    # Calculate training metrics\n    avg_loss = total_loss / len(train_loader)\n    train_acc = 100 * train_correct / train_total\n\n    # Validation phase\n    model.eval()\n    val_preds, val_labels = [], []\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            pixel_values = batch['pixel_values'].to(device) \n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids, attention_mask, pixel_values)\n            preds = torch.argmax(outputs, dim=1)\n            \n            val_preds.extend(preds.cpu().numpy())\n            val_labels.extend(labels.cpu().numpy())\n\n    # Calculate validation metrics\n    val_acc = accuracy_score(val_labels, val_preds) * 100\n    val_f1 = f1_score(val_labels, val_preds)\n    \n    # Print results\n    print(f\"Epoch {epoch+1:2d}: Loss={avg_loss:.4f} | Train Acc={train_acc:.1f}% | Val Acc={val_acc:.1f}% | F1={val_f1:.3f}\")\n    \n    # Save best model\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        # Use safe saving format for PyTorch 2.6+ compatibility\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_val_acc': float(best_val_acc),  # Convert to native Python float\n        }, model_save_path)\n        print(f\"  ‚úì New best model saved! Val Acc: {best_val_acc:.1f}%\")\n\nprint(f\"\\n=== TRAINING COMPLETED ===\")\nprint(f\"Best validation accuracy: {best_val_acc:.1f}%\")\n\n# Final test evaluation\nif len(test_df) > 0:\n    print(f\"\\n=== FINAL TEST EVALUATION ===\")\n    \n    # Load best model with PyTorch 2.6+ compatibility\n    try:\n        # Try safe loading first (recommended)\n        checkpoint = torch.load(model_save_path, map_location=device, weights_only=True)\n    except Exception as safe_load_error:\n        print(f\"‚ö†Ô∏è  Safe loading failed: {safe_load_error}\")\n        print(\"üîÑ Attempting legacy loading (weights_only=False)...\")\n        try:\n            # Fallback to legacy loading if safe loading fails\n            checkpoint = torch.load(model_save_path, map_location=device, weights_only=False)\n            print(\"‚úÖ Legacy loading successful\")\n        except Exception as legacy_error:\n            print(f\"‚ùå Both loading methods failed: {legacy_error}\")\n            print(\"üí° Skipping test evaluation due to model loading issues\")\n            checkpoint = None\n    \n    if checkpoint is not None:\n        model.load_state_dict(checkpoint['model_state_dict'])\n    if checkpoint is not None:\n        model.load_state_dict(checkpoint['model_state_dict'])\n        \n        test_dataset = SarcasmDataset(test_df, processor)\n        test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n        \n        model.eval()\n        test_preds, test_labels = [], []\n        \n        with torch.no_grad():\n            for batch in tqdm(test_loader, desc=\"Testing\"):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                pixel_values = batch['pixel_values'].to(device)\n                labels = batch['labels'].to(device)\n                \n                outputs = model(input_ids, attention_mask, pixel_values)\n                preds = torch.argmax(outputs, dim=1)\n                \n                test_preds.extend(preds.cpu().numpy())\n                test_labels.extend(labels.cpu().numpy())\n        \n        test_acc = accuracy_score(test_labels, test_preds) * 100\n        test_f1 = f1_score(test_labels, test_preds)\n        \n        print(f\"FINAL RESULTS:\")\n        print(f\"  Test Accuracy: {test_acc:.1f}%\")\n        print(f\"  Test F1-Score: {test_f1:.3f}\")\n        print(f\"  Best Val Accuracy: {best_val_acc:.1f}%\")\n    else:\n        print(\"‚ö†Ô∏è  Test evaluation skipped due to model loading issues\")\n        print(\"üí° Training completed but unable to load saved model for testing\")\nelse:\n    print(\"No test data available for final evaluation\")\n\n# Clean up GPU memory\ntorch.cuda.empty_cache()\n\n# --- GENERATE ADDITIONAL OUTPUT FILES ---\nprint(\"\\n=== GENERATING OUTPUT FILES ===\")\n\n# 1. Save training metrics and results\nresults_summary = {\n    'experiment_config': {\n        'shot_variation': SHOT_VARIATION,\n        'training_samples': len(train_df),\n        'validation_samples': len(val_df),\n        'test_samples': len(test_df),\n        'device': str(device),\n        'pytorch_version': torch.__version__\n    },\n    'training_results': {\n        'best_validation_accuracy': float(best_val_acc) if 'best_val_acc' in locals() else None,\n        'final_test_accuracy': float(test_acc) if 'test_acc' in locals() else None,\n        'final_test_f1': float(test_f1) if 'test_f1' in locals() else None\n    },\n    'model_info': {\n        'architecture': 'CLIP + Cue Learning',\n        'base_model': 'openai/clip-vit-large-patch14',\n        'text_preprocessing': 'LLM-enhanced (mistralai/mistral-nemo)'\n    }\n}\n\n# Save results as JSON\nimport json\nresults_file = f\"/kaggle/working/training_results_{SHOT_VARIATION}.json\"\nwith open(results_file, 'w') as f:\n    json.dump(results_summary, f, indent=2)\nprint(f\"‚úÖ Results saved: {results_file}\")\n\n# 2. Save model configuration and hyperparameters\nconfig_file = f\"/kaggle/working/model_config_{SHOT_VARIATION}.json\"\nmodel_config = {\n    'shot_variation': SHOT_VARIATION,\n    'model_architecture': 'CueLearningSarcasmModel',\n    'base_clip_model': 'openai/clip-vit-large-patch14',\n    'hyperparameters': {\n        'learning_rate': 2e-3,\n        'weight_decay': 1e-4,\n        'num_epochs': 30,\n        'batch_size': 2,\n        'val_batch_size': 8\n    },\n    'prompt_lengths': {\n        'text_prompts': 12,\n        'image_prompts': 20,\n        'sarcasm_prompts': 8\n    }\n}\n\nwith open(config_file, 'w') as f:\n    json.dump(model_config, f, indent=2)\nprint(f\"‚úÖ Config saved: {config_file}\")\n\n# 3. Save predictions if test evaluation was performed\nif 'test_preds' in locals() and 'test_labels' in locals():\n    predictions_df = pd.DataFrame({\n        'true_label': test_labels,\n        'predicted_label': test_preds,\n        'correct': [1 if true == pred else 0 for true, pred in zip(test_labels, test_preds)]\n    })\n    predictions_file = f\"/kaggle/working/test_predictions_{SHOT_VARIATION}.csv\"\n    predictions_df.to_csv(predictions_file, index=False)\n    print(f\"‚úÖ Predictions saved: {predictions_file}\")\n\n# 4. Save sample processed data for verification\nif len(train_df) > 0:\n    sample_data = train_df.head(10)[['id', 'text', 'processed_text', 'sarcasm']].copy()\n    sample_file = f\"/kaggle/working/sample_data_{SHOT_VARIATION}.csv\"\n    sample_data.to_csv(sample_file, index=False)\n    print(f\"‚úÖ Sample data saved: {sample_file}\")\n\n# 5. Create experiment summary text file\nsummary_file = f\"/kaggle/working/experiment_summary_{SHOT_VARIATION}.txt\"\nwith open(summary_file, 'w') as f:\n    f.write(f\"SARCASM DETECTION EXPERIMENT SUMMARY\\n\")\n    f.write(f\"===================================\\n\\n\")\n    f.write(f\"Configuration: {SHOT_VARIATION}\\n\")\n    f.write(f\"Training samples: {len(train_df)}\\n\")\n    f.write(f\"Validation samples: {len(val_df)}\\n\")\n    f.write(f\"Test samples: {len(test_df)}\\n\")\n    f.write(f\"Device: {device}\\n\")\n    f.write(f\"PyTorch version: {torch.__version__}\\n\\n\")\n    \n    if 'best_val_acc' in locals():\n        f.write(f\"Best Validation Accuracy: {best_val_acc:.2f}%\\n\")\n    if 'test_acc' in locals():\n        f.write(f\"Final Test Accuracy: {test_acc:.2f}%\\n\")\n        f.write(f\"Final Test F1-Score: {test_f1:.4f}\\n\")\n    \n    f.write(f\"\\nModel Architecture: CLIP + Cue Learning\\n\")\n    f.write(f\"Base Model: openai/clip-vit-large-patch14\\n\")\n    f.write(f\"Text Preprocessing: LLM-enhanced (mistralai/mistral-nemo)\\n\")\n\nprint(f\"‚úÖ Summary saved: {summary_file}\")\n\nprint(\"‚úì Training completed successfully!\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 9. KAGGLE EXECUTION SUMMARY ---\nprint(\"=\" * 70)\nprint(\"üéØ MULTIMODAL SARCASM DETECTION - EXECUTION SUMMARY\")\nprint(\"=\" * 70)\n\nprint(f\"üìä Dataset Information:\")\nprint(f\"   ‚Ä¢ Training samples: {len(train_df)} ({SHOT_VARIATION})\")\nprint(f\"   ‚Ä¢ Validation samples: {len(val_df) if 'val_df' in locals() else 'N/A'}\")\nprint(f\"   ‚Ä¢ Test samples: {len(test_df) if 'test_df' in locals() else 'N/A'}\")\nprint(f\"   ‚Ä¢ Data source: Separate team-preprocessed CSV files\")\n\n# Show shot configuration details\nif SHOT_VARIATION != \"all\":\n    samples_per_class = shot_configs.get(SHOT_VARIATION)\n    total_samples = samples_per_class * 2 if samples_per_class else len(train_df)\n    print(f\"   ‚Ä¢ Shot configuration: {samples_per_class} per class ‚Üí {total_samples} total training\")\nelse:\n    print(f\"   ‚Ä¢ Shot configuration: All available training data\")\n\nprint(f\"\\nü§ñ Model Configuration:\")\nprint(f\"   ‚Ä¢ Architecture: CLIP + Cue Learning (Multimodal)\")\nprint(f\"   ‚Ä¢ Device: {device}\")\nprint(f\"   ‚Ä¢ Text preprocessing: LLM-enhanced (mistralai/mistral-nemo)\")\nprint(f\"   ‚Ä¢ Image processing: CLIP visual encoder\")\n\nprint(f\"\\nüéØ Shot Learning Experiment:\")\navailable_shots = [\"16shot\", \"64shot\", \"128shot\", \"512shot\", \"1024shot\", \"all\"]\nprint(f\"   ‚Ä¢ Current configuration: {SHOT_VARIATION}\")\nprint(f\"   ‚Ä¢ Available configurations: {', '.join(available_shots)}\")\nprint(f\"   ‚Ä¢ Purpose: Compare performance across different data scales\")\nprint(f\"   ‚Ä¢ Training data: Subsampled from preprocessed train CSV\")\nprint(f\"   ‚Ä¢ Val/Test data: Full preprocessed datasets for fair evaluation\")\n\nprint(f\"\\nüìà Training Results:\")\nif 'best_val_acc' in locals():\n    print(f\"   ‚Ä¢ Best Validation Accuracy: {best_val_acc:.1f}%\")\nif 'test_acc' in locals():\n    print(f\"   ‚Ä¢ Final Test Accuracy: {test_acc:.1f}%\")\n    print(f\"   ‚Ä¢ Final Test F1-Score: {test_f1:.3f}\")\n\n# Data quality summary\nif 'missing_processed_train' in locals():\n    total_missing = missing_processed_train + missing_processed_val + missing_processed_test\n    total_samples_all = len(train_df) + len(val_df) + len(test_df)\n    quality_pct = ((total_samples_all - total_missing) / total_samples_all) * 100\n    print(f\"\\nüîç Data Quality:\")\n    print(f\"   ‚Ä¢ Preprocessed text quality: {quality_pct:.1f}% complete\")\n    print(f\"   ‚Ä¢ Missing processed entries: {total_missing}/{total_samples_all}\")\n    print(f\"   ‚Ä¢ Train processed: {len(train_df) - missing_processed_train}/{len(train_df)}\")\n    print(f\"   ‚Ä¢ Val processed: {len(val_df) - missing_processed_val}/{len(val_df)}\")\n    print(f\"   ‚Ä¢ Test processed: {len(test_df) - missing_processed_test}/{len(test_df)}\")\n\nprint(f\"\\nüíæ Output Files:\")\nkaggle_working = \"/kaggle/working\"\noutput_files = []\n\n# Enhanced file listing with categorization\nfor root, dirs, files in os.walk(kaggle_working):\n    for file in files:\n        if file.endswith(('.pth', '.csv', '.txt', '.json')):\n            filepath = os.path.join(root, file)\n            size_kb = os.path.getsize(filepath) / 1024\n            rel_path = os.path.relpath(filepath, kaggle_working)\n            \n            # Categorize files\n            if file.endswith('.pth'):\n                file_type = \"ü§ñ Model\"\n            elif file.endswith('.json'):\n                file_type = \"üìä Config/Results\"\n            elif file.endswith('.csv'):\n                file_type = \"üìã Data\"\n            elif file.endswith('.txt'):\n                file_type = \"üìù Summary\"\n            else:\n                file_type = \"üìÑ Other\"\n                \n            output_files.append((file_type, rel_path, size_kb))\n\nif output_files:\n    # Sort by file type for better organization\n    output_files.sort(key=lambda x: x[0])\n    \n    print(\"   Expected output files for download:\")\n    for file_type, rel_path, size_kb in output_files:\n        print(f\"   {file_type}: {rel_path} ({size_kb:.1f} KB)\")\n        \n    print(f\"\\n   üì¶ Total files generated: {len(output_files)}\")\n    print(f\"   üìÅ All files available in: /kaggle/working/\")\nelse:\n    print(\"   ‚Ä¢ No output files found\")\n\n# Additional file expectations\nprint(f\"\\nüéØ Expected Output Files for {SHOT_VARIATION}:\")\nexpected_files = [\n    f\"ü§ñ best_model.pth - Trained model weights\",\n    f\"üìä training_results_{SHOT_VARIATION}.json - Experiment results\",\n    f\"üìä model_config_{SHOT_VARIATION}.json - Model configuration\", \n    f\"üìã test_predictions_{SHOT_VARIATION}.csv - Test predictions\",\n    f\"üìã sample_data_{SHOT_VARIATION}.csv - Sample processed data\",\n    f\"üìù experiment_summary_{SHOT_VARIATION}.txt - Human-readable summary\"\n]\n\nfor expected in expected_files:\n    print(f\"   ‚Ä¢ {expected}\")\n\nprint(f\"\\nüí° Download Instructions:\")\nprint(f\"   1. Go to Output tab in Kaggle\")\nprint(f\"   2. Download all files from /kaggle/working/\")\nprint(f\"   3. Use best_model.pth for inference\")\nprint(f\"   4. Share results JSON with team for comparison\")\n\nprint(f\"\\nüìÅ Input Files Used:\")\nprint(f\"   ‚Ä¢ Training: {len(train_df)} samples from team preprocessing\")\nprint(f\"   ‚Ä¢ Validation: {len(val_df)} samples from team preprocessing\")  \nprint(f\"   ‚Ä¢ Test: {len(test_df)} samples from team preprocessing\")\nprint(f\"   ‚Ä¢ File structure: 3 separate CSV files (no merging)\")\n\nprint(f\"\\nüöÄ Next Steps:\")\nprint(f\"   ‚Ä¢ Try different SHOT_VARIATION values to compare performance\")\nprint(f\"   ‚Ä¢ Upload model results to team for analysis\")\nprint(f\"   ‚Ä¢ Consider ensemble methods for final submission\")\nprint(f\"   ‚Ä¢ Each team member can focus on their specific preprocessing task\")\n\nprint(f\"\\nüéØ Workflow Benefits:\")\nprint(f\"   ‚Ä¢ No merge required: Direct use of separate CSV files\")\nprint(f\"   ‚Ä¢ Parallel preprocessing: Each person works independently\")\nprint(f\"   ‚Ä¢ Easy debugging: Issues isolated per dataset\")\nprint(f\"   ‚Ä¢ Flexible deployment: Can use any combination of processed files\")\n\nprint(f\"\\n‚úÖ Experiment completed successfully!\")\nprint(f\"Shot: {SHOT_VARIATION} | Device: {device} | Separate file workflow\")\n\n# --- PYTORCH VERSION & COMPATIBILITY CHECK ---\nprint(\"=== PYTORCH VERSION & COMPATIBILITY CHECK ===\")\nprint(f\"PyTorch version: {torch.__version__}\")\n\n# Check PyTorch version for compatibility\nimport torch\npytorch_version = torch.__version__\nmajor_version = int(pytorch_version.split('.')[0])\nminor_version = int(pytorch_version.split('.')[1])\n\nif major_version >= 2 and minor_version >= 6:\n    print(f\"‚úÖ PyTorch {pytorch_version} detected (2.6+)\")\n    print(\"üîß Using safe model loading by default\")\n    print(\"üí° If loading fails, will fallback to legacy mode\")\nelse:\n    print(f\"‚úÖ PyTorch {pytorch_version} detected (< 2.6)\")\n    print(\"üîß Using legacy model loading\")\n\n# Check if we have numpy compatibility issues\ntry:\n    import numpy as np\n    print(f\"NumPy version: {np.__version__}\")\n    \n    # Test numpy scalar compatibility\n    test_scalar = np.float64(1.0)\n    print(f\"‚úÖ NumPy scalar test passed: {type(test_scalar)}\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è  NumPy compatibility issue: {e}\")\n\nprint(\"=\" * 50)\n\n# --- MODEL SAVING HELPER FUNCTIONS ---\ndef save_model_safe(model, optimizer, epoch, val_acc, filepath):\n    \"\"\"Save model with PyTorch 2.6+ compatibility\"\"\"\n    try:\n        # Use native Python types to avoid unpickling issues\n        checkpoint = {\n            'epoch': int(epoch),\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_val_acc': float(val_acc),  # Convert to native float\n            'pytorch_version': torch.__version__\n        }\n        \n        torch.save(checkpoint, filepath)\n        print(f\"‚úÖ Model saved successfully: {filepath}\")\n        return True\n        \n    except Exception as e:\n        print(f\"‚ùå Error saving model: {e}\")\n        return False\n\ndef load_model_safe(filepath, device='cpu'):\n    \"\"\"Load model with PyTorch 2.6+ compatibility\"\"\"\n    try:\n        # Try safe loading first (PyTorch 2.6+ default)\n        checkpoint = torch.load(filepath, map_location=device, weights_only=True)\n        print(\"‚úÖ Safe model loading successful\")\n        return checkpoint\n        \n    except Exception as safe_error:\n        print(f\"‚ö†Ô∏è  Safe loading failed: {safe_error}\")\n        \n        try:\n            # Fallback to legacy loading\n            print(\"üîÑ Attempting legacy loading...\")\n            checkpoint = torch.load(filepath, map_location=device, weights_only=False)\n            print(\"‚úÖ Legacy model loading successful\")\n            return checkpoint\n            \n        except Exception as legacy_error:\n            print(f\"‚ùå Legacy loading also failed: {legacy_error}\")\n            return None\n\nprint(\"‚úÖ Model saving/loading helper functions defined\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}